{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccf4446-7343-42c6-9aed-8ffc22178853",
   "metadata": {},
   "source": [
    "# Introduction To Relax\n",
    "\n",
    "In this tutorial we would be getting a Relay model and then converting it into a Relax model using the [Relay -> Relax converter](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/testing/relay_translator.py). We will also get a high level view into the Relax IR representation of the model and capture some basic concepts.\n",
    "\n",
    "This tutorial will guide you to bring your Relay model into  Relax, walk you through the Relax IR representation of the model, and then build and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7af6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import tvm\n",
    "from tvm.relay import testing\n",
    "from tvm import relax, relay\n",
    "from tvm.relax.testing import relay_translator\n",
    "from tvm.runtime import vm as vm_rt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3b743",
   "metadata": {},
   "source": [
    "### Import a Relay Model\n",
    "Lets get a Relay model from library. For this tutorial we have chosen MLP but feel free to play around with any Relay model of your choice. You can also use any other model in Relay for this tutorial. We will import the model with unknown batch dimension by passing `batch_size=relay.Any()`.\n",
    "\n",
    "We can dump the Relay IR representation of the model. It uses 10 Relay operators such as `nn.batch_flatten`, `nn.dense`, `nn.relu`, etc. Since we imported model with dynamic batch dimension, the `%data` input to the model has shape `(?, 1, 28, 28)` and the output of the model is `(?, 10)` because the model has 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7959e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%data: Tensor[(?, 1, 28, 28), float32] /* ty=Tensor[(?, 1, 28, 28), float32] */, %fc1_weight: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, %fc1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %fc2_weight: Tensor[(64, 128), float32] /* ty=Tensor[(64, 128), float32] */, %fc2_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %fc3_weight: Tensor[(10, 64), float32] /* ty=Tensor[(10, 64), float32] */, %fc3_bias: Tensor[(10), float32] /* ty=Tensor[(10), float32] */) -> Tensor[(?, 10), float32] {\n",
      "  %0 = nn.batch_flatten(%data) /* ty=Tensor[(?, 784), float32] */;\n",
      "  %1 = nn.dense(%0, %fc1_weight, units=128) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %2 = nn.bias_add(%1, %fc1_bias, axis=-1) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %3 = nn.relu(%2) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %4 = nn.dense(%3, %fc2_weight, units=64) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %5 = nn.bias_add(%4, %fc2_bias, axis=-1) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %6 = nn.relu(%5) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %7 = nn.dense(%6, %fc3_weight, units=10) /* ty=Tensor[(?, 10), float32] */;\n",
      "  %8 = nn.bias_add(%7, %fc3_bias, axis=-1) /* ty=Tensor[(?, 10), float32] */;\n",
      "  nn.softmax(%8) /* ty=Tensor[(?, 10), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dshape = (1, 28, 28)\n",
    "relay_mod, params_dict = testing.mlp.get_workload(batch_size=relay.Any())\n",
    "print(relay_mod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5559ab7",
   "metadata": {},
   "source": [
    "### Convert Relay IR to Relax IR\n",
    "\n",
    "Now let's convert the imported Relay module to Relax module. Relax provides a simple utility [Relay -> Relax converter](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/testing/relay_translator.py) to convert any legacy Relay module into Relax. The converter directly lowers the relay operations to TIR implementations, so we need to provide the `target` to the converter. Let's import for `llvm` target.\n",
    "\n",
    "We can dump the Relax IR module and take a look.\n",
    "\n",
    "The Relay to Relax converter translates the Relay model into a dataflow block with calls to TIR implementation of Relay operators. So the Relax IR module would have a high-level function (`main`) and TensorIR functions, one for each of the operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a129f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@add = primfn(var_rxplaceholder: handle, var_rxplaceholder_1: handle, var_T_add: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"add\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder: Buffer(rxplaceholder_2: Pointer(global float32), float32, [d: int64, 128i64], []),\n",
      "             rxplaceholder_1: Buffer(rxplaceholder_3: Pointer(global float32), float32, [1, 128i64], []),\n",
      "             T_add: Buffer(T_add_1: Pointer(global float32), float32, [d, 128i64], [])}\n",
      "  buffer_map = {var_rxplaceholder: rxplaceholder, var_rxplaceholder_1: rxplaceholder_1, var_T_add: T_add} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0: int64, 0i64, d) {\n",
      "      for (i1: int64, 0i64, 128i64) {\n",
      "        block([d, 128i64], \"T_add\") as [ax0, ax1] {\n",
      "          bind(ax0, i0)\n",
      "          bind(ax1, i1)\n",
      "          tir.reads([rxplaceholder[ax0, ax1], rxplaceholder_1[0i64, ax1]])\n",
      "          tir.writes([T_add[ax0, ax1]])\n",
      "          T_add[ax0, ax1] = (rxplaceholder[ax0, ax1] + rxplaceholder_1[0i64, ax1])\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@add1 = primfn(var_rxplaceholder_2: handle, var_rxplaceholder_3: handle, var_T_add_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"add1\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_4: Buffer(rxplaceholder_6: Pointer(global float32), float32, [d_1: int64, 64i64], []),\n",
      "             rxplaceholder_5: Buffer(rxplaceholder_7: Pointer(global float32), float32, [1, 64i64], []),\n",
      "             T_add_2: Buffer(T_add_3: Pointer(global float32), float32, [d_1, 64i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_2: rxplaceholder_4, var_rxplaceholder_3: rxplaceholder_5, var_T_add_1: T_add_2} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_1: int64, 0i64, d_1) {\n",
      "      for (i1_1: int64, 0i64, 64i64) {\n",
      "        block([d_1, 64i64], \"T_add\") as [ax0_1, ax1_1] {\n",
      "          bind(ax0_1, i0_1)\n",
      "          bind(ax1_1, i1_1)\n",
      "          tir.reads([rxplaceholder_4[ax0_1, ax1_1], rxplaceholder_5[0i64, ax1_1]])\n",
      "          tir.writes([T_add_2[ax0_1, ax1_1]])\n",
      "          T_add_2[ax0_1, ax1_1] = (rxplaceholder_4[ax0_1, ax1_1] + rxplaceholder_5[0i64, ax1_1])\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@add2 = primfn(var_rxplaceholder_4: handle, var_rxplaceholder_5: handle, var_T_add_2: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"add2\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_8: Buffer(rxplaceholder_10: Pointer(global float32), float32, [d_2: int64, 10i64], []),\n",
      "             rxplaceholder_9: Buffer(rxplaceholder_11: Pointer(global float32), float32, [1, 10i64], []),\n",
      "             T_add_4: Buffer(T_add_5: Pointer(global float32), float32, [d_2, 10i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_4: rxplaceholder_8, var_rxplaceholder_5: rxplaceholder_9, var_T_add_2: T_add_4} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_2: int64, 0i64, d_2) {\n",
      "      for (i1_2: int64, 0i64, 10i64) {\n",
      "        block([d_2, 10i64], \"T_add\") as [ax0_2, ax1_2] {\n",
      "          bind(ax0_2, i0_2)\n",
      "          bind(ax1_2, i1_2)\n",
      "          tir.reads([rxplaceholder_8[ax0_2, ax1_2], rxplaceholder_9[0i64, ax1_2]])\n",
      "          tir.writes([T_add_4[ax0_2, ax1_2]])\n",
      "          T_add_4[ax0_2, ax1_2] = (rxplaceholder_8[ax0_2, ax1_2] + rxplaceholder_9[0i64, ax1_2])\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@batch_flatten = primfn(var_rxplaceholder_6: handle, var_tensor: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"batch_flatten\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_12: Buffer(rxplaceholder_13: Pointer(global float32), float32, [d_3: int64, 1i64, 28i64, 28i64], []),\n",
      "             tensor: Buffer(tensor_1: Pointer(global float32), float32, [d_3, 784i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_6: rxplaceholder_12, var_tensor: tensor} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_3: int64, 0i64, d_3) {\n",
      "      for (i1_3: int64, 0i64, 784i64) {\n",
      "        block([d_3, 784i64], \"tensor\") as [ax0_3, ax1_3] {\n",
      "          bind(ax0_3, i0_3)\n",
      "          bind(ax1_3, i1_3)\n",
      "          tir.reads([rxplaceholder_12[ax0_3, 0i64, floordiv(floormod(ax1_3, 784i64), 28i64), floormod(ax1_3, 28i64)]])\n",
      "          tir.writes([tensor[ax0_3, ax1_3]])\n",
      "          tensor[ax0_3, ax1_3] = rxplaceholder_12[ax0_3, 0i64, floordiv(floormod(ax1_3, 784i64), 28i64), floormod(ax1_3, 28i64)]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@dense = primfn(var_rxplaceholder_7: handle, var_rxplaceholder_8: handle, var_T_matmul_NT: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"dense\", \"tir.noalias\": True, \"layout_free_placeholders\": [1]}\n",
      "  buffers = {rxplaceholder_14: Buffer(rxplaceholder_16: Pointer(global float32), float32, [d_4: int64, 784i64], []),\n",
      "             rxplaceholder_15: Buffer(rxplaceholder_17: Pointer(global float32), float32, [128i64, 784i64], []),\n",
      "             T_matmul_NT: Buffer(T_matmul_NT_1: Pointer(global float32), float32, [d_4, 128i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_7: rxplaceholder_14, var_rxplaceholder_8: rxplaceholder_15, var_T_matmul_NT: T_matmul_NT} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_4: int64, 0i64, d_4) {\n",
      "      for (i1_4: int64, 0i64, 128i64) {\n",
      "        for (i2: int64, 0i64, 784i64) {\n",
      "          block([d_4, 128i64, tir.reduce_axis(0, 784i64)], \"T_matmul_NT\") as [i, j, k] {\n",
      "            bind(i, i0_4)\n",
      "            bind(j, i1_4)\n",
      "            bind(k, i2)\n",
      "            tir.reads([rxplaceholder_14[i, k], rxplaceholder_15[j, k]])\n",
      "            tir.writes([T_matmul_NT[i, j]])\n",
      "            with init() {\n",
      "              T_matmul_NT[i, j] = 0f32\n",
      "            }\n",
      "            T_matmul_NT[i, j] = (T_matmul_NT[i, j] + (rxplaceholder_14[i, k]*rxplaceholder_15[j, k]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@dense1 = primfn(var_rxplaceholder_9: handle, var_rxplaceholder_10: handle, var_T_matmul_NT_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"dense1\", \"tir.noalias\": True, \"layout_free_placeholders\": [1]}\n",
      "  buffers = {rxplaceholder_18: Buffer(rxplaceholder_20: Pointer(global float32), float32, [d_5: int64, 128i64], []),\n",
      "             rxplaceholder_19: Buffer(rxplaceholder_21: Pointer(global float32), float32, [64i64, 128i64], []),\n",
      "             T_matmul_NT_2: Buffer(T_matmul_NT_3: Pointer(global float32), float32, [d_5, 64i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_9: rxplaceholder_18, var_rxplaceholder_10: rxplaceholder_19, var_T_matmul_NT_1: T_matmul_NT_2} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_5: int64, 0i64, d_5) {\n",
      "      for (i1_5: int64, 0i64, 64i64) {\n",
      "        for (i2_1: int64, 0i64, 128i64) {\n",
      "          block([d_5, 64i64, tir.reduce_axis(0, 128i64)], \"T_matmul_NT\") as [i_1, j_1, k_1] {\n",
      "            bind(i_1, i0_5)\n",
      "            bind(j_1, i1_5)\n",
      "            bind(k_1, i2_1)\n",
      "            tir.reads([rxplaceholder_18[i_1, k_1], rxplaceholder_19[j_1, k_1]])\n",
      "            tir.writes([T_matmul_NT_2[i_1, j_1]])\n",
      "            with init() {\n",
      "              T_matmul_NT_2[i_1, j_1] = 0f32\n",
      "            }\n",
      "            T_matmul_NT_2[i_1, j_1] = (T_matmul_NT_2[i_1, j_1] + (rxplaceholder_18[i_1, k_1]*rxplaceholder_19[j_1, k_1]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@dense2 = primfn(var_rxplaceholder_11: handle, var_rxplaceholder_12: handle, var_T_matmul_NT_2: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"dense2\", \"tir.noalias\": True, \"layout_free_placeholders\": [1]}\n",
      "  buffers = {rxplaceholder_22: Buffer(rxplaceholder_24: Pointer(global float32), float32, [d_6: int64, 64i64], []),\n",
      "             rxplaceholder_23: Buffer(rxplaceholder_25: Pointer(global float32), float32, [10i64, 64i64], []),\n",
      "             T_matmul_NT_4: Buffer(T_matmul_NT_5: Pointer(global float32), float32, [d_6, 10i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_11: rxplaceholder_22, var_rxplaceholder_12: rxplaceholder_23, var_T_matmul_NT_2: T_matmul_NT_4} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_6: int64, 0i64, d_6) {\n",
      "      for (i1_6: int64, 0i64, 10i64) {\n",
      "        for (i2_2: int64, 0i64, 64i64) {\n",
      "          block([d_6, 10i64, tir.reduce_axis(0, 64i64)], \"T_matmul_NT\") as [i_2, j_2, k_2] {\n",
      "            bind(i_2, i0_6)\n",
      "            bind(j_2, i1_6)\n",
      "            bind(k_2, i2_2)\n",
      "            tir.reads([rxplaceholder_22[i_2, k_2], rxplaceholder_23[j_2, k_2]])\n",
      "            tir.writes([T_matmul_NT_4[i_2, j_2]])\n",
      "            with init() {\n",
      "              T_matmul_NT_4[i_2, j_2] = 0f32\n",
      "            }\n",
      "            T_matmul_NT_4[i_2, j_2] = (T_matmul_NT_4[i_2, j_2] + (rxplaceholder_22[i_2, k_2]*rxplaceholder_23[j_2, k_2]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@expand_dims = primfn(var_rxplaceholder_13: handle, var_T_expand_dims: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"expand_dims\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_26: Buffer(rxplaceholder_27: Pointer(global float32), float32, [128i64], []),\n",
      "             T_expand_dims: Buffer(T_expand_dims_1: Pointer(global float32), float32, [1, 128i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_13: rxplaceholder_26, var_T_expand_dims: T_expand_dims} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_7: int32, 0, 1) {\n",
      "      for (i1_7: int64, 0i64, 128i64) {\n",
      "        block([1, 128i64], \"T_expand_dims\") as [ax0_4, ax1_4] {\n",
      "          bind(ax0_4, i0_7)\n",
      "          bind(ax1_4, i1_7)\n",
      "          tir.reads([rxplaceholder_26[ax1_4]])\n",
      "          tir.writes([T_expand_dims[ax0_4, ax1_4]])\n",
      "          T_expand_dims[ax0_4, ax1_4] = rxplaceholder_26[ax1_4]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@expand_dims1 = primfn(var_rxplaceholder_14: handle, var_T_expand_dims_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"expand_dims1\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_28: Buffer(rxplaceholder_29: Pointer(global float32), float32, [64i64], []),\n",
      "             T_expand_dims_2: Buffer(T_expand_dims_3: Pointer(global float32), float32, [1, 64i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_14: rxplaceholder_28, var_T_expand_dims_1: T_expand_dims_2} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_8: int32, 0, 1) {\n",
      "      for (i1_8: int64, 0i64, 64i64) {\n",
      "        block([1, 64i64], \"T_expand_dims\") as [ax0_5, ax1_5] {\n",
      "          bind(ax0_5, i0_8)\n",
      "          bind(ax1_5, i1_8)\n",
      "          tir.reads([rxplaceholder_28[ax1_5]])\n",
      "          tir.writes([T_expand_dims_2[ax0_5, ax1_5]])\n",
      "          T_expand_dims_2[ax0_5, ax1_5] = rxplaceholder_28[ax1_5]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@expand_dims2 = primfn(var_rxplaceholder_15: handle, var_T_expand_dims_2: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"expand_dims2\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_30: Buffer(rxplaceholder_31: Pointer(global float32), float32, [10i64], []),\n",
      "             T_expand_dims_4: Buffer(T_expand_dims_5: Pointer(global float32), float32, [1, 10i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_15: rxplaceholder_30, var_T_expand_dims_2: T_expand_dims_4} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_9: int32, 0, 1) {\n",
      "      for (i1_9: int64, 0i64, 10i64) {\n",
      "        block([1, 10i64], \"T_expand_dims\") as [ax0_6, ax1_6] {\n",
      "          bind(ax0_6, i0_9)\n",
      "          bind(ax1_6, i1_9)\n",
      "          tir.reads([rxplaceholder_30[ax1_6]])\n",
      "          tir.writes([T_expand_dims_4[ax0_6, ax1_6]])\n",
      "          T_expand_dims_4[ax0_6, ax1_6] = rxplaceholder_30[ax1_6]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@relax.function\n",
      "def main(data: Tensor((d, 1, 28, 28), \"float32\"), fc1_weight: Tensor((128, 784), \"float32\"), fc1_bias: Tensor((128,), \"float32\"), fc2_weight: Tensor((64, 128), \"float32\"), fc2_bias: Tensor((64,), \"float32\"), fc3_weight: Tensor((10, 64), \"float32\"), fc3_bias: Tensor((10,), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "    # block 0\n",
      "    with relax.dataflow():\n",
      "        lv = relax.call_tir(batch_flatten, (data,), (d, 784), dtype=\"float32\")\n",
      "        lv1 = relax.call_tir(dense, (lv, fc1_weight), (d, 128), dtype=\"float32\")\n",
      "        lv2 = relax.call_tir(expand_dims, (fc1_bias,), (1, 128), dtype=\"float32\")\n",
      "        lv3 = relax.call_tir(add, (lv1, lv2), (d, 128), dtype=\"float32\")\n",
      "        lv4 = relax.call_tir(relu, (lv3,), (d, 128), dtype=\"float32\")\n",
      "        lv5 = relax.call_tir(dense1, (lv4, fc2_weight), (d, 64), dtype=\"float32\")\n",
      "        lv6 = relax.call_tir(expand_dims1, (fc2_bias,), (1, 64), dtype=\"float32\")\n",
      "        lv7 = relax.call_tir(add1, (lv5, lv6), (d, 64), dtype=\"float32\")\n",
      "        lv8 = relax.call_tir(relu1, (lv7,), (d, 64), dtype=\"float32\")\n",
      "        lv9 = relax.call_tir(dense2, (lv8, fc3_weight), (d, 10), dtype=\"float32\")\n",
      "        lv10 = relax.call_tir(expand_dims2, (fc3_bias,), (1, 10), dtype=\"float32\")\n",
      "        lv11 = relax.call_tir(add2, (lv9, lv10), (d, 10), dtype=\"float32\")\n",
      "        lv12 = relax.call_tir(softmax, (lv11,), (d, 10), dtype=\"float32\")\n",
      "        gv: Tensor((d, 10), \"float32\") = lv12\n",
      "        relax.output(gv)\n",
      "    return gv\n",
      "\n",
      "\n",
      "@relu = primfn(var_rxplaceholder_16: handle, var_T_relu: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"relu\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_32: Buffer(rxplaceholder_33: Pointer(global float32), float32, [d_7: int64, 128i64], []),\n",
      "             T_relu: Buffer(T_relu_1: Pointer(global float32), float32, [d_7, 128i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_16: rxplaceholder_32, var_T_relu: T_relu} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_10: int64, 0i64, d_7) {\n",
      "      for (i1_10: int64, 0i64, 128i64) {\n",
      "        block([d_7, 128i64], \"T_relu\") as [ax0_7, ax1_7] {\n",
      "          bind(ax0_7, i0_10)\n",
      "          bind(ax1_7, i1_10)\n",
      "          tir.reads([rxplaceholder_32[ax0_7, ax1_7]])\n",
      "          tir.writes([T_relu[ax0_7, ax1_7]])\n",
      "          T_relu[ax0_7, ax1_7] = max(rxplaceholder_32[ax0_7, ax1_7], 0f32)\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@relu1 = primfn(var_rxplaceholder_17: handle, var_T_relu_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"relu1\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_34: Buffer(rxplaceholder_35: Pointer(global float32), float32, [d_8: int64, 64i64], []),\n",
      "             T_relu_2: Buffer(T_relu_3: Pointer(global float32), float32, [d_8, 64i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_17: rxplaceholder_34, var_T_relu_1: T_relu_2} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0_11: int64, 0i64, d_8) {\n",
      "      for (i1_11: int64, 0i64, 64i64) {\n",
      "        block([d_8, 64i64], \"T_relu\") as [ax0_8, ax1_8] {\n",
      "          bind(ax0_8, i0_11)\n",
      "          bind(ax1_8, i1_11)\n",
      "          tir.reads([rxplaceholder_34[ax0_8, ax1_8]])\n",
      "          tir.writes([T_relu_2[ax0_8, ax1_8]])\n",
      "          T_relu_2[ax0_8, ax1_8] = max(rxplaceholder_34[ax0_8, ax1_8], 0f32)\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "@softmax = primfn(var_rxplaceholder_18: handle, var_T_softmax_norm: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"softmax\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder_36: Buffer(rxplaceholder_37: Pointer(global float32), float32, [d_9: int64, 10i64], []),\n",
      "             T_softmax_norm: Buffer(T_softmax_norm_1: Pointer(global float32), float32, [d_9, 10i64], [])}\n",
      "  buffer_map = {var_rxplaceholder_18: rxplaceholder_36, var_T_softmax_norm: T_softmax_norm} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    T_softmax_maxelem = alloc_buffer(float32[d_9])\n",
      "    T_softmax_exp = alloc_buffer(float32[d_9, 10i64])\n",
      "    T_softmax_expsum = alloc_buffer(float32[d_9])\n",
      "     {\n",
      "      for (i0_12: int64, 0i64, d_9) {\n",
      "        for (i1_12: int64, 0i64, 10i64) {\n",
      "          block([d_9, tir.reduce_axis(0, 10i64)], \"T_softmax_maxelem\") as [i0_13, k_3] {\n",
      "            bind(i0_13, i0_12)\n",
      "            bind(k_3, i1_12)\n",
      "            tir.reads([rxplaceholder_36[i0_13, k_3]])\n",
      "            tir.writes([T_softmax_maxelem[i0_13]])\n",
      "            with init() {\n",
      "              T_softmax_maxelem[i0_13] = -3.40282e+38f32\n",
      "            }\n",
      "            T_softmax_maxelem[i0_13] = max(T_softmax_maxelem[i0_13], rxplaceholder_36[i0_13, k_3])\n",
      "        }\n",
      "      }\n",
      "      for (i0_14: int64, 0i64, d_9) {\n",
      "        for (i1_13: int64, 0i64, 10i64) {\n",
      "          block([d_9, 10i64], \"T_softmax_exp\") as [i0_15, i1_14] {\n",
      "            bind(i0_15, i0_14)\n",
      "            bind(i1_14, i1_13)\n",
      "            tir.reads([rxplaceholder_36[i0_15, i1_14], T_softmax_maxelem[i0_15]])\n",
      "            tir.writes([T_softmax_exp[i0_15, i1_14]])\n",
      "            T_softmax_exp[i0_15, i1_14] = @tir.exp((rxplaceholder_36[i0_15, i1_14] - T_softmax_maxelem[i0_15]), dtype=float32)\n",
      "        }\n",
      "      }\n",
      "      for (i0_16: int64, 0i64, d_9) {\n",
      "        for (i1_15: int64, 0i64, 10i64) {\n",
      "          block([d_9, tir.reduce_axis(0, 10i64)], \"T_softmax_expsum\") as [i0_17, k_4] {\n",
      "            bind(i0_17, i0_16)\n",
      "            bind(k_4, i1_15)\n",
      "            tir.reads([T_softmax_exp[i0_17, k_4]])\n",
      "            tir.writes([T_softmax_expsum[i0_17]])\n",
      "            with init() {\n",
      "              T_softmax_expsum[i0_17] = 0f32\n",
      "            }\n",
      "            T_softmax_expsum[i0_17] = (T_softmax_expsum[i0_17] + T_softmax_exp[i0_17, k_4])\n",
      "        }\n",
      "      }\n",
      "      for (i0_18: int64, 0i64, d_9) {\n",
      "        for (i1_16: int64, 0i64, 10i64) {\n",
      "          block([d_9, 10i64], \"T_softmax_norm\") as [i0_19, i1_17] {\n",
      "            bind(i0_19, i0_18)\n",
      "            bind(i1_17, i1_16)\n",
      "            tir.reads([T_softmax_exp[i0_19, i1_17], T_softmax_expsum[i0_19]])\n",
      "            tir.writes([T_softmax_norm[i0_19, i1_17]])\n",
      "            tir.attrs({\"axis\": 1})\n",
      "            T_softmax_norm[i0_19, i1_17] = (T_softmax_exp[i0_19, i1_17] / T_softmax_expsum[i0_19])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = tvm.target.Target(\"llvm\")\n",
    "relax_mod = relay_translator.from_relay(relay_mod[\"main\"], target)\n",
    "\n",
    "# To look at the entire Relax IR module you can dump it using the following code.\n",
    "# print(relax_mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f8a8c",
   "metadata": {},
   "source": [
    "### Relax and TensorIR Functions\n",
    "Woah! That's a big IR module but don't worry it will all make sense very soon. We can observe that the IR module contains a number of TIR functions and one Relax function. The Relax function has the decorator `@relax.function` and the TIR functions start with `@<func_name> = primfn(...`\n",
    "\n",
    "Let's look at the Relax function more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e94499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@relax.function\n",
      "def main(data: Tensor((d, 1, 28, 28), \"float32\"), fc1_weight: Tensor((128, 784), \"float32\"), fc1_bias: Tensor((128,), \"float32\"), fc2_weight: Tensor((64, 128), \"float32\"), fc2_bias: Tensor((64,), \"float32\"), fc3_weight: Tensor((10, 64), \"float32\"), fc3_bias: Tensor((10,), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "    # block 0\n",
      "    with relax.dataflow():\n",
      "        lv = relax.call_tir(batch_flatten, (data,), (d, 784), dtype=\"float32\")\n",
      "        lv1 = relax.call_tir(dense, (lv, fc1_weight), (d, 128), dtype=\"float32\")\n",
      "        lv2 = relax.call_tir(expand_dims, (fc1_bias,), (1, 128), dtype=\"float32\")\n",
      "        lv3 = relax.call_tir(add, (lv1, lv2), (d, 128), dtype=\"float32\")\n",
      "        lv4 = relax.call_tir(relu, (lv3,), (d, 128), dtype=\"float32\")\n",
      "        lv5 = relax.call_tir(dense1, (lv4, fc2_weight), (d, 64), dtype=\"float32\")\n",
      "        lv6 = relax.call_tir(expand_dims1, (fc2_bias,), (1, 64), dtype=\"float32\")\n",
      "        lv7 = relax.call_tir(add1, (lv5, lv6), (d, 64), dtype=\"float32\")\n",
      "        lv8 = relax.call_tir(relu1, (lv7,), (d, 64), dtype=\"float32\")\n",
      "        lv9 = relax.call_tir(dense2, (lv8, fc3_weight), (d, 10), dtype=\"float32\")\n",
      "        lv10 = relax.call_tir(expand_dims2, (fc3_bias,), (1, 10), dtype=\"float32\")\n",
      "        lv11 = relax.call_tir(add2, (lv9, lv10), (d, 10), dtype=\"float32\")\n",
      "        lv12 = relax.call_tir(softmax, (lv11,), (d, 10), dtype=\"float32\")\n",
      "        gv: Tensor((d, 10), \"float32\") = lv12\n",
      "        relax.output(gv)\n",
      "    return gv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(relax_mod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2cd26",
   "metadata": {},
   "source": [
    "The Relax functions in Relax IR module are decorated with `@relax.function`. The signature of the function contains shape and dtypes of the parameters and output of the function. This is all pretty similar to Relay module so far. However, it has some fundamental new features which we briefly discuss below. They are covered in greater detail in future tutorials.\n",
    "\n",
    "### Dataflow Block\n",
    "\n",
    "You can observe the that the model code is encapsulated in a `with relax.dataflow()` construct. Relax enforces some guarantees within this construct such as, all the operations under the dataflow block are side-effect-free and do not contain advanced control flows(such as if-then-else) or nested scopes. A dataflow block can effectively be viewed as a computational graph embedded in the program. Note that most of the binding variables(lv, lv1, lv2, lv3) within the dataflow block are \"local\", which means they are only visible within the block. These variables can be viewed as \"internal nodes\" of the computational graph. We can mark a variable as output(`gv`), in which case the variable will be visible in later part of the program. These output variables can be viewed as output nodes in the computational graph.\n",
    "\n",
    "Note that return gv is outside of the dataflow block. Everything that is outside of a dataflow block can have side effects. So we cannot perform optimizations such as reordering these bindings according to topological order unless we do more careful analysis We expect most of the optimizations will happen at the dataflow block level. These optimizations can be done by ML engineers who are familiar with the computational graph concept. The ability to isolate and represent effectful components also provides opportunities for more advanced optimizations for the places that need them.\n",
    "\n",
    "### Direct Interaction with TensorIR\n",
    "\n",
    "In Relax high-level IR can directly interact and call into lower-level TensorIR (also PackedFunc, but this example does not cover that). For example, `lv = relax.call_tir(batch_flatten, (data,), (d, 784), dtype=\"float32\")`. This calls into the TensorIR function `batch_flatten`. The arguments to the TensorIR function are `data` and the output is expected to be a tensor of shape `(d, 784)` and dtype `float32`. This unlocks multiple opportunities, including, but not limited to:\n",
    "\n",
    "* Incrementally lower different parts of the program using different strategies.\n",
    "* Allow automation to take a `call_tir` to TensorIR, perform optimization and rewrite into multiple `call_tir` note that informs layout rewriting decisions to the high-level.\n",
    "* Bring BYOC flow as a natural part of transformation(by transforming part of the graph into call of opaque packed functions).\n",
    "\n",
    "\n",
    "### Symbolic Shape Dimensions\n",
    "The unknown dimension in Relay module has been replaced with a symbolic dimension in Relax module. `%data: Tensor[(?, 1, 28, 28), float32]` in Relay was translated to `data: Tensor((d, 1, 28, 28), \"float32\")` in Relax. The unknown batch dimension is captured by symbolic integer TIR variable `d`. Symbolic dimensions are not limited to batch dimension in Relax and can be used in place of any dimension in the shape of any tensor.\n",
    "\n",
    "The benefits of symbolic dimensions over unknown (`?`) dimensions is that it can express relationships between different Tensors in the model. For example, in Relax program we know that `data` and `lv` share the same first dimension `d`. This can lead to better memory planning for dynamic shape models. This information is lost in the Relay program with unknown shapes. We will cover symbolic shapes in more detail in future tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46e4de",
   "metadata": {},
   "source": [
    "The other functions in the Relax module are TensorIR functions. For example, take a look at `batch_flatten` using `print(relax_mod[\"batch_flatten\"])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae25d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(var_rxplaceholder: handle, var_tensor: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"batch_flatten\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder: Buffer(rxplaceholder_1: Pointer(global float32), float32, [d: int64, 1i64, 28i64, 28i64], []),\n",
      "             tensor: Buffer(tensor_1: Pointer(global float32), float32, [d, 784i64], [])}\n",
      "  buffer_map = {var_rxplaceholder: rxplaceholder, var_tensor: tensor} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0: int64, 0i64, d) {\n",
      "      for (i1: int64, 0i64, 784i64) {\n",
      "        block([d, 784i64], \"tensor\") as [ax0, ax1] {\n",
      "          bind(ax0, i0)\n",
      "          bind(ax1, i1)\n",
      "          tir.reads([rxplaceholder[ax0, 0i64, floordiv(floormod(ax1, 784i64), 28i64), floormod(ax1, 28i64)]])\n",
      "          tir.writes([tensor[ax0, ax1]])\n",
      "          tensor[ax0, ax1] = rxplaceholder[ax0, 0i64, floordiv(floormod(ax1, 784i64), 28i64), floormod(ax1, 28i64)]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(relax_mod[\"batch_flatten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f9b26",
   "metadata": {},
   "source": [
    "## Compile Relax Module\n",
    "\n",
    "Relax has a simple API to compile the Relax module to VM executable. We can dump the VM executable as text using `ex.stats()` and `ex.as_text()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458e00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relax VM executable statistics:\n",
      "  Constant pool (# 91): [shapetuple[30], shapetuple[0, 1, 2, 3], shapetuple[4, 5], shapetuple[6], shapetuple[7, 8], shapetuple[9], shapetuple[10, 11], shapetuple[12], shapetuple[13], float32, shapetuple[0, 14], shapetuple[0, 14], float32, shapetuple[0, 14], shapetuple[0, 14], shapetuple[15], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[512], float32, shapetuple[1, 128], float32, shapetuple[18], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[19], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[20], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[256], float32, shapetuple[1, 64], float32, shapetuple[23], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[24], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[25], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[40], float32, shapetuple[1, 10], float32, shapetuple[28], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[29], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[0, 10]]\n",
      "  Globals (#1): [main]\n",
      "  Packed functions (#33): [vm.builtin.alloc_shape_heap, vm.builtin.shape_of, vm.builtin.store_shape, shape_func, vm.builtin.load_shape, vm.builtin.alloc_storage, shape_func1, vm.builtin.alloc_tensor, batch_flatten, shape_func2, shape_func3, dense, expand_dims, shape_func4, add, shape_func5, relu, shape_func6, shape_func7, dense1, expand_dims1, shape_func8, add1, shape_func9, relu1, shape_func10, shape_func11, dense2, expand_dims2, shape_func12, add2, shape_func13, softmax]\n",
      "\n",
      "@main:\n",
      "  call  vm.builtin.alloc_shape_heap in: %vm, c[0]    dst: %7\n",
      "  call  vm.builtin.shape_of in: %0           dst: %8\n",
      "  call  vm.builtin.store_shape in: %8, %7, c[1] dst: %9\n",
      "  call  vm.builtin.shape_of in: %1           dst: %10\n",
      "  call  vm.builtin.store_shape in: %10, %7, c[2] dst: %11\n",
      "  call  vm.builtin.shape_of in: %2           dst: %12\n",
      "  call  vm.builtin.store_shape in: %12, %7, c[3] dst: %13\n",
      "  call  vm.builtin.shape_of in: %3           dst: %14\n",
      "  call  vm.builtin.store_shape in: %14, %7, c[4] dst: %15\n",
      "  call  vm.builtin.shape_of in: %4           dst: %16\n",
      "  call  vm.builtin.store_shape in: %16, %7, c[5] dst: %17\n",
      "  call  vm.builtin.shape_of in: %5           dst: %18\n",
      "  call  vm.builtin.store_shape in: %18, %7, c[6] dst: %19\n",
      "  call  vm.builtin.shape_of in: %6           dst: %20\n",
      "  call  vm.builtin.store_shape in: %20, %7, c[7] dst: %21\n",
      "  call  shape_func       in: %7           dst: %22\n",
      "  call  vm.builtin.load_shape in: %7, c[8]     dst: %23\n",
      "  call  vm.builtin.alloc_storage in: %vm, %23, i0, c[9] dst: %24\n",
      "  call  shape_func1      in: %7           dst: %25\n",
      "  call  vm.builtin.load_shape in: %7, c[10]    dst: %26\n",
      "  call  shape_func1      in: %7           dst: %27\n",
      "  call  vm.builtin.load_shape in: %7, c[11]    dst: %28\n",
      "  call  vm.builtin.alloc_tensor in: %24, i0, %26, c[12] dst: %29\n",
      "  call  shape_func1      in: %7           dst: %30\n",
      "  call  vm.builtin.load_shape in: %7, c[13]    dst: %31\n",
      "  call  batch_flatten    in: %0, %29      dst: %32\n",
      "  call  shape_func1      in: %7           dst: %33\n",
      "  call  vm.builtin.load_shape in: %7, c[14]    dst: %34\n",
      "  call  shape_func2      in: %7           dst: %35\n",
      "  call  vm.builtin.load_shape in: %7, c[15]    dst: %36\n",
      "  call  vm.builtin.alloc_storage in: %vm, %36, i0, c[16] dst: %37\n",
      "  call  shape_func3      in: %7           dst: %38\n",
      "  call  vm.builtin.load_shape in: %7, c[17]    dst: %39\n",
      "  call  shape_func3      in: %7           dst: %40\n",
      "  call  vm.builtin.load_shape in: %7, c[18]    dst: %41\n",
      "  call  vm.builtin.alloc_tensor in: %37, i0, %39, c[19] dst: %42\n",
      "  call  shape_func3      in: %7           dst: %43\n",
      "  call  vm.builtin.load_shape in: %7, c[20]    dst: %44\n",
      "  call  dense            in: %29, %1, %42 dst: %45\n",
      "  call  shape_func3      in: %7           dst: %46\n",
      "  call  vm.builtin.load_shape in: %7, c[21]    dst: %47\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[22], i0, c[23] dst: %48\n",
      "  call  vm.builtin.alloc_tensor in: %48, i0, c[24], c[25] dst: %49\n",
      "  call  expand_dims      in: %2, %49      dst: %50\n",
      "  call  shape_func4      in: %7           dst: %51\n",
      "  call  vm.builtin.load_shape in: %7, c[26]    dst: %52\n",
      "  call  vm.builtin.alloc_storage in: %vm, %52, i0, c[27] dst: %53\n",
      "  call  shape_func3      in: %7           dst: %54\n",
      "  call  vm.builtin.load_shape in: %7, c[28]    dst: %55\n",
      "  call  shape_func3      in: %7           dst: %56\n",
      "  call  vm.builtin.load_shape in: %7, c[29]    dst: %57\n",
      "  call  vm.builtin.alloc_tensor in: %53, i0, %55, c[30] dst: %58\n",
      "  call  shape_func3      in: %7           dst: %59\n",
      "  call  vm.builtin.load_shape in: %7, c[31]    dst: %60\n",
      "  call  add              in: %42, %49, %58 dst: %61\n",
      "  call  shape_func3      in: %7           dst: %62\n",
      "  call  vm.builtin.load_shape in: %7, c[32]    dst: %63\n",
      "  call  shape_func5      in: %7           dst: %64\n",
      "  call  vm.builtin.load_shape in: %7, c[33]    dst: %65\n",
      "  call  vm.builtin.alloc_storage in: %vm, %65, i0, c[34] dst: %66\n",
      "  call  shape_func3      in: %7           dst: %67\n",
      "  call  vm.builtin.load_shape in: %7, c[35]    dst: %68\n",
      "  call  shape_func3      in: %7           dst: %69\n",
      "  call  vm.builtin.load_shape in: %7, c[36]    dst: %70\n",
      "  call  vm.builtin.alloc_tensor in: %66, i0, %68, c[37] dst: %71\n",
      "  call  shape_func3      in: %7           dst: %72\n",
      "  call  vm.builtin.load_shape in: %7, c[38]    dst: %73\n",
      "  call  relu             in: %58, %71     dst: %74\n",
      "  call  shape_func3      in: %7           dst: %75\n",
      "  call  vm.builtin.load_shape in: %7, c[39]    dst: %76\n",
      "  call  shape_func6      in: %7           dst: %77\n",
      "  call  vm.builtin.load_shape in: %7, c[40]    dst: %78\n",
      "  call  vm.builtin.alloc_storage in: %vm, %78, i0, c[41] dst: %79\n",
      "  call  shape_func7      in: %7           dst: %80\n",
      "  call  vm.builtin.load_shape in: %7, c[42]    dst: %81\n",
      "  call  shape_func7      in: %7           dst: %82\n",
      "  call  vm.builtin.load_shape in: %7, c[43]    dst: %83\n",
      "  call  vm.builtin.alloc_tensor in: %79, i0, %81, c[44] dst: %84\n",
      "  call  shape_func7      in: %7           dst: %85\n",
      "  call  vm.builtin.load_shape in: %7, c[45]    dst: %86\n",
      "  call  dense1           in: %71, %3, %84 dst: %87\n",
      "  call  shape_func7      in: %7           dst: %88\n",
      "  call  vm.builtin.load_shape in: %7, c[46]    dst: %89\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[47], i0, c[48] dst: %90\n",
      "  call  vm.builtin.alloc_tensor in: %90, i0, c[49], c[50] dst: %91\n",
      "  call  expand_dims1     in: %4, %91      dst: %92\n",
      "  call  shape_func8      in: %7           dst: %93\n",
      "  call  vm.builtin.load_shape in: %7, c[51]    dst: %94\n",
      "  call  vm.builtin.alloc_storage in: %vm, %94, i0, c[52] dst: %95\n",
      "  call  shape_func7      in: %7           dst: %96\n",
      "  call  vm.builtin.load_shape in: %7, c[53]    dst: %97\n",
      "  call  shape_func7      in: %7           dst: %98\n",
      "  call  vm.builtin.load_shape in: %7, c[54]    dst: %99\n",
      "  call  vm.builtin.alloc_tensor in: %95, i0, %97, c[55] dst: %100\n",
      "  call  shape_func7      in: %7           dst: %101\n",
      "  call  vm.builtin.load_shape in: %7, c[56]    dst: %102\n",
      "  call  add1             in: %84, %91, %100 dst: %103\n",
      "  call  shape_func7      in: %7           dst: %104\n",
      "  call  vm.builtin.load_shape in: %7, c[57]    dst: %105\n",
      "  call  shape_func9      in: %7           dst: %106\n",
      "  call  vm.builtin.load_shape in: %7, c[58]    dst: %107\n",
      "  call  vm.builtin.alloc_storage in: %vm, %107, i0, c[59] dst: %108\n",
      "  call  shape_func7      in: %7           dst: %109\n",
      "  call  vm.builtin.load_shape in: %7, c[60]    dst: %110\n",
      "  call  shape_func7      in: %7           dst: %111\n",
      "  call  vm.builtin.load_shape in: %7, c[61]    dst: %112\n",
      "  call  vm.builtin.alloc_tensor in: %108, i0, %110, c[62] dst: %113\n",
      "  call  shape_func7      in: %7           dst: %114\n",
      "  call  vm.builtin.load_shape in: %7, c[63]    dst: %115\n",
      "  call  relu1            in: %100, %113   dst: %116\n",
      "  call  shape_func7      in: %7           dst: %117\n",
      "  call  vm.builtin.load_shape in: %7, c[64]    dst: %118\n",
      "  call  shape_func10     in: %7           dst: %119\n",
      "  call  vm.builtin.load_shape in: %7, c[65]    dst: %120\n",
      "  call  vm.builtin.alloc_storage in: %vm, %120, i0, c[66] dst: %121\n",
      "  call  shape_func11     in: %7           dst: %122\n",
      "  call  vm.builtin.load_shape in: %7, c[67]    dst: %123\n",
      "  call  shape_func11     in: %7           dst: %124\n",
      "  call  vm.builtin.load_shape in: %7, c[68]    dst: %125\n",
      "  call  vm.builtin.alloc_tensor in: %121, i0, %123, c[69] dst: %126\n",
      "  call  shape_func11     in: %7           dst: %127\n",
      "  call  vm.builtin.load_shape in: %7, c[70]    dst: %128\n",
      "  call  dense2           in: %113, %5, %126 dst: %129\n",
      "  call  shape_func11     in: %7           dst: %130\n",
      "  call  vm.builtin.load_shape in: %7, c[71]    dst: %131\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[72], i0, c[73] dst: %132\n",
      "  call  vm.builtin.alloc_tensor in: %132, i0, c[74], c[75] dst: %133\n",
      "  call  expand_dims2     in: %6, %133     dst: %134\n",
      "  call  shape_func12     in: %7           dst: %135\n",
      "  call  vm.builtin.load_shape in: %7, c[76]    dst: %136\n",
      "  call  vm.builtin.alloc_storage in: %vm, %136, i0, c[77] dst: %137\n",
      "  call  shape_func11     in: %7           dst: %138\n",
      "  call  vm.builtin.load_shape in: %7, c[78]    dst: %139\n",
      "  call  shape_func11     in: %7           dst: %140\n",
      "  call  vm.builtin.load_shape in: %7, c[79]    dst: %141\n",
      "  call  vm.builtin.alloc_tensor in: %137, i0, %139, c[80] dst: %142\n",
      "  call  shape_func11     in: %7           dst: %143\n",
      "  call  vm.builtin.load_shape in: %7, c[81]    dst: %144\n",
      "  call  add2             in: %126, %133, %142 dst: %145\n",
      "  call  shape_func11     in: %7           dst: %146\n",
      "  call  vm.builtin.load_shape in: %7, c[82]    dst: %147\n",
      "  call  shape_func13     in: %7           dst: %148\n",
      "  call  vm.builtin.load_shape in: %7, c[83]    dst: %149\n",
      "  call  vm.builtin.alloc_storage in: %vm, %149, i0, c[84] dst: %150\n",
      "  call  shape_func11     in: %7           dst: %151\n",
      "  call  vm.builtin.load_shape in: %7, c[85]    dst: %152\n",
      "  call  shape_func11     in: %7           dst: %153\n",
      "  call  vm.builtin.load_shape in: %7, c[86]    dst: %154\n",
      "  call  vm.builtin.alloc_tensor in: %150, i0, %152, c[87] dst: %155\n",
      "  call  shape_func11     in: %7           dst: %156\n",
      "  call  vm.builtin.load_shape in: %7, c[88]    dst: %157\n",
      "  call  softmax          in: %142, %155   dst: %158\n",
      "  call  shape_func11     in: %7           dst: %159\n",
      "  call  vm.builtin.load_shape in: %7, c[89]    dst: %160\n",
      "  call  shape_func11     in: %7           dst: %161\n",
      "  call  vm.builtin.load_shape in: %7, c[90]    dst: %162\n",
      "  ret   ret %155\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get params and input for the module\n",
    "batch_size = 2\n",
    "shape = (batch_size, *dshape)\n",
    "data = tvm.nd.array(np.random.rand(*shape).astype(np.float32))\n",
    "params = list(params_dict.values())\n",
    "\n",
    "# Build the Relax IRModule\n",
    "ex = relax.vm.build(relax_mod, target)\n",
    "\n",
    "print(ex.stats())\n",
    "print(ex.as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a89a94",
   "metadata": {},
   "source": [
    "## Execute Relax IR module\n",
    "\n",
    "Now the compiled relax VM executable can be run and we can compare the results with Relay for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd1d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 784), 'float32'), ('TENSOR', (128, 784), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 128), 'float32'), ('TENSOR', (64, 128), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 64), 'float32'), ('TENSOR', (10, 64), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "/home/prakalp/repos/relax/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  \"target_host parameter is going to be deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "res = vm[\"main\"](data, *params)\n",
    "\n",
    "# check correctness by comparing with relay result\n",
    "exe = relay.vm.compile(relay_mod, target)\n",
    "relay_vm = vm_rt.VirtualMachine(exe, tvm.cpu())\n",
    "inputs = [data] + params\n",
    "expected_output = relay_vm.run(*inputs)\n",
    "tvm.testing.assert_allclose(res.numpy(), expected_output.numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc43b24afbafa24450c433caa80e216064e86ab300af4b30503108fd4e5dd3a0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
