{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccf4446-7343-42c6-9aed-8ffc22178853",
   "metadata": {},
   "source": [
    "# Introduction To Relax\n",
    "\n",
    "In this tutorial, we will get a Relay model and then convert it into a Relax model using the [Relay -> Relax converter](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/testing/relay_translator.py). This will give us a chance to review the Relax IR at a high level and capture some basic concepts.\n",
    "\n",
    "This tutorial will show you how to bring your Relay model into Relax and run it, as well as explain the basics of the Relax IR representation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7af6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import tvm\n",
    "from tvm.relay import testing\n",
    "from tvm import relax, relay\n",
    "from tvm.relax.testing import relay_translator\n",
    "from tvm.runtime import vm as vm_rt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3b743",
   "metadata": {},
   "source": [
    "### Import a Relay Model\n",
    "Let us begin with a Relay model. For this tutorial, we will use the multilayer preceptron (MLP), for its compactness and simplicity, but feel free to play around with any Relay model of your choice. We will import the model with a dynamic batch dimension by passing `batch_size=relay.Any()`. Below, we print the model's representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7959e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%data: Tensor[(?, 1, 28, 28), float32] /* ty=Tensor[(?, 1, 28, 28), float32] */, %fc1_weight: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, %fc1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %fc2_weight: Tensor[(64, 128), float32] /* ty=Tensor[(64, 128), float32] */, %fc2_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %fc3_weight: Tensor[(10, 64), float32] /* ty=Tensor[(10, 64), float32] */, %fc3_bias: Tensor[(10), float32] /* ty=Tensor[(10), float32] */) -> Tensor[(?, 10), float32] {\n",
      "  %0 = nn.batch_flatten(%data) /* ty=Tensor[(?, 784), float32] */;\n",
      "  %1 = nn.dense(%0, %fc1_weight, units=128) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %2 = nn.bias_add(%1, %fc1_bias, axis=-1) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %3 = nn.relu(%2) /* ty=Tensor[(?, 128), float32] */;\n",
      "  %4 = nn.dense(%3, %fc2_weight, units=64) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %5 = nn.bias_add(%4, %fc2_bias, axis=-1) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %6 = nn.relu(%5) /* ty=Tensor[(?, 64), float32] */;\n",
      "  %7 = nn.dense(%6, %fc3_weight, units=10) /* ty=Tensor[(?, 10), float32] */;\n",
      "  %8 = nn.bias_add(%7, %fc3_bias, axis=-1) /* ty=Tensor[(?, 10), float32] */;\n",
      "  nn.softmax(%8) /* ty=Tensor[(?, 10), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dshape = (1, 28, 28)\n",
    "relay_mod, params_dict = testing.mlp.get_workload(batch_size=relay.Any())\n",
    "print(relay_mod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b8d73",
   "metadata": {},
   "source": [
    "\n",
    "Let's inspect the Relay IR representation of the model. It uses 10 Relay operators such as `nn.batch_flatten`, `nn.dense`, `nn.relu`, etc., applied in feedforward fashion (the output of the previous operator is the input to the next). Since we imported model with dynamic batch dimension, the `%data` input to the model has shape `(?, 1, 28, 28)` and the output of the model is `(?, 10)`, corresponding to 10 classes of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5559ab7",
   "metadata": {},
   "source": [
    "### Converting Relay to Relax\n",
    "\n",
    "Now let's convert the imported Relay module into a Relax module; we can do this automatically thanks to the built-in [Relay -> Relax converter](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/testing/relay_translator.py) utility. The resulting Relax module consists of a Relax function corresponding to the Relay implementation above, as well as TIR implementations of the operators called in the Relay model (Relax supports calling TIR inline). Note that the TIR calls need a target for code generation; in this case, we will lower to LLVM.\n",
    "\n",
    "Let's inspect the resulting module, starting with the entrypoint function, `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a129f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@relax.function\n",
      "def main(data: Tensor((d, 1, 28, 28), \"float32\"), fc1_weight: Tensor((128, 784), \"float32\"), fc1_bias: Tensor((128,), \"float32\"), fc2_weight: Tensor((64, 128), \"float32\"), fc2_bias: Tensor((64,), \"float32\"), fc3_weight: Tensor((10, 64), \"float32\"), fc3_bias: Tensor((10,), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "    # block 0\n",
      "    with relax.dataflow():\n",
      "        lv = relax.call_tir(batch_flatten, (data,), (d, 784), dtype=\"float32\")\n",
      "        lv1 = relax.call_tir(dense, (lv, fc1_weight), (d, 128), dtype=\"float32\")\n",
      "        lv2 = relax.call_tir(expand_dims, (fc1_bias,), (1, 128), dtype=\"float32\")\n",
      "        lv3 = relax.call_tir(add, (lv1, lv2), (d, 128), dtype=\"float32\")\n",
      "        lv4 = relax.call_tir(relu, (lv3,), (d, 128), dtype=\"float32\")\n",
      "        lv5 = relax.call_tir(dense1, (lv4, fc2_weight), (d, 64), dtype=\"float32\")\n",
      "        lv6 = relax.call_tir(expand_dims1, (fc2_bias,), (1, 64), dtype=\"float32\")\n",
      "        lv7 = relax.call_tir(add1, (lv5, lv6), (d, 64), dtype=\"float32\")\n",
      "        lv8 = relax.call_tir(relu1, (lv7,), (d, 64), dtype=\"float32\")\n",
      "        lv9 = relax.call_tir(dense2, (lv8, fc3_weight), (d, 10), dtype=\"float32\")\n",
      "        lv10 = relax.call_tir(expand_dims2, (fc3_bias,), (1, 10), dtype=\"float32\")\n",
      "        lv11 = relax.call_tir(add2, (lv9, lv10), (d, 10), dtype=\"float32\")\n",
      "        lv12 = relax.call_tir(softmax, (lv11,), (d, 10), dtype=\"float32\")\n",
      "        gv: Tensor((d, 10), \"float32\") = lv12\n",
      "        relax.output(gv)\n",
      "    return gv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = tvm.target.Target(\"llvm\")\n",
    "relax_mod = relay_translator.from_relay(relay_mod[\"main\"], target)\n",
    "\n",
    "# To look at the entire Relax IR module you can dump it using the following code.\n",
    "# print(relax_mod)\n",
    "\n",
    "print(relax_mod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ac119",
   "metadata": {},
   "source": [
    "#### Relax Functions\n",
    "\n",
    "The function `main()` is printed in TVMScript format and begins with the `@relax.function` decorator. Despite the difference in format, however, the Relax `main()` function resembles its Relay counterpart in many ways, though it also shows off some fundamental new features. The following sections will discuss several of the differences and what they mean. Many will be covered in further detail in upcoming tutorials.\n",
    "\n",
    "#### Dataflow Block\n",
    "\n",
    "We may observe the that the model code is encapsulated in a `with relax.dataflow()` construct. Relax enforces certain guarantees within a dataflow block: namely, that all the operations within the block are side effect-free and that there are no control flow (e.g., if-then-else) constructs or nested scopes. Since the original Relay model did not have any side effects, branching control flow, or nested scopes, all of the functionality of the original model can be safely contained in this block.\n",
    "\n",
    "A dataflow block can effectively be viewed as a computational graph embedded in the program. Note that most of the binding variables (`lv`, `lv1`, `lv2`, `lv3`) within the dataflow block are \"local\", which means they are only visible within the block. These variables can be viewed as \"internal nodes\" of the computational graph. We can mark a variable as output (as is done with `gv`), in which case the variable will be visible in later part of the program. These output variables can be viewed as output nodes in the computational graph.\n",
    "\n",
    "Note that `return gv` is outside of the dataflow block. Code that is outside of a dataflow block can have side effects and so requires further analysis to determine if optimizations like reordering bindings are safe. We expect most of the optimizations to be implemented on dataflow blocks, where they can take advantage of the safety guarantees. These optimizations can be done by ML engineers who are familiar with computational graphs. The ability to isolate and represent effectful components also provides opportunities for more advanced optimizations for the places that need them.\n",
    "\n",
    "#### Symbolic Shape Dimensions\n",
    "\n",
    "Another difference between the two models can be observed in the type signatures. The \"any\" dimension in Relay module has been replaced with a symbolic dimension in Relax module; `%data: Tensor[(?, 1, 28, 28), float32]` in Relay was translated to `data: Tensor((d, 1, 28, 28), \"float32\")` in Relax. Though this example has a symbolic batch dimension, Relax permits any dimension in a tensor shape to be symbolic.\n",
    "\n",
    "Relax's use of symbolic dimensions has an important benefit over Relay's single wildcard `?` dimension: it can express relationships between different Tensors in the model. For example, in Relax program we know that `data` and `lv` share the same first dimension `d`; by contrast, each use of `?` in Relay can match any dimension. The more precise information in Relax can allow for better memory planning for models with dynamic shapes and otherwise represent more correctness properties than Relay's \"any\" dimension. We will cover symbolic shapes in more detail in future tutorial.\n",
    "\n",
    "#### Direct Interaction with TensorIR\n",
    "\n",
    "One last difference between the models is that the Relay model is calling into a set of predefined operators whereas the Relax model is dirrectly calling into TensorIR implementations of the corresponding operators. In Relax, the high-level IR can directly interact with and call into the lower-level TensorIR (and also invoke arbitrary `PackedFunc`s, but this example does not make use of this).\n",
    "\n",
    "For example, the line `lv = relax.call_tir(batch_flatten, (data,), (d, 784), dtype=\"float32\")` is directly calling into the TensorIR function `batch_flatten`. The arguments to the TensorIR function are `data` and the output is expected to be a tensor of shape `(d, 784)` and dtype `float32`. We can observe below that the Relax module indeed contains a TIR definition for `batch_flatten`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca45ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(var_rxplaceholder: handle, var_tensor: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"batch_flatten\", \"tir.noalias\": True}\n",
      "  buffers = {rxplaceholder: Buffer(rxplaceholder_1: Pointer(global float32), float32, [d: int64, 1i64, 28i64, 28i64], []),\n",
      "             tensor: Buffer(tensor_1: Pointer(global float32), float32, [d, 784i64], [])}\n",
      "  buffer_map = {var_rxplaceholder: rxplaceholder, var_tensor: tensor} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    for (i0: int64, 0i64, d) {\n",
      "      for (i1: int64, 0i64, 784i64) {\n",
      "        block([d, 784i64], \"tensor\") as [ax0, ax1] {\n",
      "          bind(ax0, i0)\n",
      "          bind(ax1, i1)\n",
      "          tir.reads([rxplaceholder[ax0, 0i64, floordiv(floormod(ax1, 784i64), 28i64), floormod(ax1, 28i64)]])\n",
      "          tir.writes([tensor[ax0, ax1]])\n",
      "          tensor[ax0, ax1] = rxplaceholder[ax0, 0i64, floordiv(floormod(ax1, 784i64), 28i64), floormod(ax1, 28i64)]\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(relax_mod[\"batch_flatten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9e63c",
   "metadata": {},
   "source": [
    "Unlike Relax functions, TIR functions start with `@<func_name> = primfn(...`.\n",
    "\n",
    "The ability to call TensorIR functions directly is very convenient for experimenting with new operators, compared to the relative complexity of registering new operators in Relay. It also provides many further advantages in compilation, such as the following capabilities (among others):\n",
    "\n",
    "* Incrementally lowering different parts of the program using different strategies\n",
    "* Communicating layout rewriting and transformations at the TIR level back to the high-level IR to inform further optimizations\n",
    "* Easier incorporation of the BYOC compilation flow into the lower levels of the stack (by transforming part of the graph into call of opaque packed functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f9b26",
   "metadata": {},
   "source": [
    "## Compiling a Relax Module\n",
    "\n",
    "Relax has a simple API to compile the Relax module to VM executable, similar to the existing VM compilation for Relay modules. We can dump the VM executable as text using `ex.stats()` and `ex.as_text()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458e00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relax VM executable statistics:\n",
      "  Constant pool (# 91): [shapetuple[30], shapetuple[0, 1, 2, 3], shapetuple[4, 5], shapetuple[6], shapetuple[7, 8], shapetuple[9], shapetuple[10, 11], shapetuple[12], shapetuple[13], float32, shapetuple[0, 14], shapetuple[0, 14], float32, shapetuple[0, 14], shapetuple[0, 14], shapetuple[15], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[512], float32, shapetuple[1, 128], float32, shapetuple[18], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[19], float32, shapetuple[0, 4], shapetuple[0, 4], float32, shapetuple[0, 4], shapetuple[0, 4], shapetuple[20], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[256], float32, shapetuple[1, 64], float32, shapetuple[23], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[24], float32, shapetuple[0, 7], shapetuple[0, 7], float32, shapetuple[0, 7], shapetuple[0, 7], shapetuple[25], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[40], float32, shapetuple[1, 10], float32, shapetuple[28], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[29], float32, shapetuple[0, 10], shapetuple[0, 10], float32, shapetuple[0, 10], shapetuple[0, 10], shapetuple[0, 10]]\n",
      "  Globals (#1): [main]\n",
      "  Packed functions (#33): [vm.builtin.alloc_shape_heap, vm.builtin.shape_of, vm.builtin.store_shape, shape_func, vm.builtin.load_shape, vm.builtin.alloc_storage, shape_func1, vm.builtin.alloc_tensor, batch_flatten, shape_func2, shape_func3, dense, expand_dims, shape_func4, add, shape_func5, relu, shape_func6, shape_func7, dense1, expand_dims1, shape_func8, add1, shape_func9, relu1, shape_func10, shape_func11, dense2, expand_dims2, shape_func12, add2, shape_func13, softmax]\n",
      "\n",
      "@main:\n",
      "  call  vm.builtin.alloc_shape_heap in: %vm, c[0]    dst: %7\n",
      "  call  vm.builtin.shape_of in: %0           dst: %8\n",
      "  call  vm.builtin.store_shape in: %8, %7, c[1] dst: %9\n",
      "  call  vm.builtin.shape_of in: %1           dst: %10\n",
      "  call  vm.builtin.store_shape in: %10, %7, c[2] dst: %11\n",
      "  call  vm.builtin.shape_of in: %2           dst: %12\n",
      "  call  vm.builtin.store_shape in: %12, %7, c[3] dst: %13\n",
      "  call  vm.builtin.shape_of in: %3           dst: %14\n",
      "  call  vm.builtin.store_shape in: %14, %7, c[4] dst: %15\n",
      "  call  vm.builtin.shape_of in: %4           dst: %16\n",
      "  call  vm.builtin.store_shape in: %16, %7, c[5] dst: %17\n",
      "  call  vm.builtin.shape_of in: %5           dst: %18\n",
      "  call  vm.builtin.store_shape in: %18, %7, c[6] dst: %19\n",
      "  call  vm.builtin.shape_of in: %6           dst: %20\n",
      "  call  vm.builtin.store_shape in: %20, %7, c[7] dst: %21\n",
      "  call  shape_func       in: %7           dst: %22\n",
      "  call  vm.builtin.load_shape in: %7, c[8]     dst: %23\n",
      "  call  vm.builtin.alloc_storage in: %vm, %23, i0, c[9] dst: %24\n",
      "  call  shape_func1      in: %7           dst: %25\n",
      "  call  vm.builtin.load_shape in: %7, c[10]    dst: %26\n",
      "  call  shape_func1      in: %7           dst: %27\n",
      "  call  vm.builtin.load_shape in: %7, c[11]    dst: %28\n",
      "  call  vm.builtin.alloc_tensor in: %24, i0, %26, c[12] dst: %29\n",
      "  call  shape_func1      in: %7           dst: %30\n",
      "  call  vm.builtin.load_shape in: %7, c[13]    dst: %31\n",
      "  call  batch_flatten    in: %0, %29      dst: %32\n",
      "  call  shape_func1      in: %7           dst: %33\n",
      "  call  vm.builtin.load_shape in: %7, c[14]    dst: %34\n",
      "  call  shape_func2      in: %7           dst: %35\n",
      "  call  vm.builtin.load_shape in: %7, c[15]    dst: %36\n",
      "  call  vm.builtin.alloc_storage in: %vm, %36, i0, c[16] dst: %37\n",
      "  call  shape_func3      in: %7           dst: %38\n",
      "  call  vm.builtin.load_shape in: %7, c[17]    dst: %39\n",
      "  call  shape_func3      in: %7           dst: %40\n",
      "  call  vm.builtin.load_shape in: %7, c[18]    dst: %41\n",
      "  call  vm.builtin.alloc_tensor in: %37, i0, %39, c[19] dst: %42\n",
      "  call  shape_func3      in: %7           dst: %43\n",
      "  call  vm.builtin.load_shape in: %7, c[20]    dst: %44\n",
      "  call  dense            in: %29, %1, %42 dst: %45\n",
      "  call  shape_func3      in: %7           dst: %46\n",
      "  call  vm.builtin.load_shape in: %7, c[21]    dst: %47\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[22], i0, c[23] dst: %48\n",
      "  call  vm.builtin.alloc_tensor in: %48, i0, c[24], c[25] dst: %49\n",
      "  call  expand_dims      in: %2, %49      dst: %50\n",
      "  call  shape_func4      in: %7           dst: %51\n",
      "  call  vm.builtin.load_shape in: %7, c[26]    dst: %52\n",
      "  call  vm.builtin.alloc_storage in: %vm, %52, i0, c[27] dst: %53\n",
      "  call  shape_func3      in: %7           dst: %54\n",
      "  call  vm.builtin.load_shape in: %7, c[28]    dst: %55\n",
      "  call  shape_func3      in: %7           dst: %56\n",
      "  call  vm.builtin.load_shape in: %7, c[29]    dst: %57\n",
      "  call  vm.builtin.alloc_tensor in: %53, i0, %55, c[30] dst: %58\n",
      "  call  shape_func3      in: %7           dst: %59\n",
      "  call  vm.builtin.load_shape in: %7, c[31]    dst: %60\n",
      "  call  add              in: %42, %49, %58 dst: %61\n",
      "  call  shape_func3      in: %7           dst: %62\n",
      "  call  vm.builtin.load_shape in: %7, c[32]    dst: %63\n",
      "  call  shape_func5      in: %7           dst: %64\n",
      "  call  vm.builtin.load_shape in: %7, c[33]    dst: %65\n",
      "  call  vm.builtin.alloc_storage in: %vm, %65, i0, c[34] dst: %66\n",
      "  call  shape_func3      in: %7           dst: %67\n",
      "  call  vm.builtin.load_shape in: %7, c[35]    dst: %68\n",
      "  call  shape_func3      in: %7           dst: %69\n",
      "  call  vm.builtin.load_shape in: %7, c[36]    dst: %70\n",
      "  call  vm.builtin.alloc_tensor in: %66, i0, %68, c[37] dst: %71\n",
      "  call  shape_func3      in: %7           dst: %72\n",
      "  call  vm.builtin.load_shape in: %7, c[38]    dst: %73\n",
      "  call  relu             in: %58, %71     dst: %74\n",
      "  call  shape_func3      in: %7           dst: %75\n",
      "  call  vm.builtin.load_shape in: %7, c[39]    dst: %76\n",
      "  call  shape_func6      in: %7           dst: %77\n",
      "  call  vm.builtin.load_shape in: %7, c[40]    dst: %78\n",
      "  call  vm.builtin.alloc_storage in: %vm, %78, i0, c[41] dst: %79\n",
      "  call  shape_func7      in: %7           dst: %80\n",
      "  call  vm.builtin.load_shape in: %7, c[42]    dst: %81\n",
      "  call  shape_func7      in: %7           dst: %82\n",
      "  call  vm.builtin.load_shape in: %7, c[43]    dst: %83\n",
      "  call  vm.builtin.alloc_tensor in: %79, i0, %81, c[44] dst: %84\n",
      "  call  shape_func7      in: %7           dst: %85\n",
      "  call  vm.builtin.load_shape in: %7, c[45]    dst: %86\n",
      "  call  dense1           in: %71, %3, %84 dst: %87\n",
      "  call  shape_func7      in: %7           dst: %88\n",
      "  call  vm.builtin.load_shape in: %7, c[46]    dst: %89\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[47], i0, c[48] dst: %90\n",
      "  call  vm.builtin.alloc_tensor in: %90, i0, c[49], c[50] dst: %91\n",
      "  call  expand_dims1     in: %4, %91      dst: %92\n",
      "  call  shape_func8      in: %7           dst: %93\n",
      "  call  vm.builtin.load_shape in: %7, c[51]    dst: %94\n",
      "  call  vm.builtin.alloc_storage in: %vm, %94, i0, c[52] dst: %95\n",
      "  call  shape_func7      in: %7           dst: %96\n",
      "  call  vm.builtin.load_shape in: %7, c[53]    dst: %97\n",
      "  call  shape_func7      in: %7           dst: %98\n",
      "  call  vm.builtin.load_shape in: %7, c[54]    dst: %99\n",
      "  call  vm.builtin.alloc_tensor in: %95, i0, %97, c[55] dst: %100\n",
      "  call  shape_func7      in: %7           dst: %101\n",
      "  call  vm.builtin.load_shape in: %7, c[56]    dst: %102\n",
      "  call  add1             in: %84, %91, %100 dst: %103\n",
      "  call  shape_func7      in: %7           dst: %104\n",
      "  call  vm.builtin.load_shape in: %7, c[57]    dst: %105\n",
      "  call  shape_func9      in: %7           dst: %106\n",
      "  call  vm.builtin.load_shape in: %7, c[58]    dst: %107\n",
      "  call  vm.builtin.alloc_storage in: %vm, %107, i0, c[59] dst: %108\n",
      "  call  shape_func7      in: %7           dst: %109\n",
      "  call  vm.builtin.load_shape in: %7, c[60]    dst: %110\n",
      "  call  shape_func7      in: %7           dst: %111\n",
      "  call  vm.builtin.load_shape in: %7, c[61]    dst: %112\n",
      "  call  vm.builtin.alloc_tensor in: %108, i0, %110, c[62] dst: %113\n",
      "  call  shape_func7      in: %7           dst: %114\n",
      "  call  vm.builtin.load_shape in: %7, c[63]    dst: %115\n",
      "  call  relu1            in: %100, %113   dst: %116\n",
      "  call  shape_func7      in: %7           dst: %117\n",
      "  call  vm.builtin.load_shape in: %7, c[64]    dst: %118\n",
      "  call  shape_func10     in: %7           dst: %119\n",
      "  call  vm.builtin.load_shape in: %7, c[65]    dst: %120\n",
      "  call  vm.builtin.alloc_storage in: %vm, %120, i0, c[66] dst: %121\n",
      "  call  shape_func11     in: %7           dst: %122\n",
      "  call  vm.builtin.load_shape in: %7, c[67]    dst: %123\n",
      "  call  shape_func11     in: %7           dst: %124\n",
      "  call  vm.builtin.load_shape in: %7, c[68]    dst: %125\n",
      "  call  vm.builtin.alloc_tensor in: %121, i0, %123, c[69] dst: %126\n",
      "  call  shape_func11     in: %7           dst: %127\n",
      "  call  vm.builtin.load_shape in: %7, c[70]    dst: %128\n",
      "  call  dense2           in: %113, %5, %126 dst: %129\n",
      "  call  shape_func11     in: %7           dst: %130\n",
      "  call  vm.builtin.load_shape in: %7, c[71]    dst: %131\n",
      "  call  vm.builtin.alloc_storage in: %vm, c[72], i0, c[73] dst: %132\n",
      "  call  vm.builtin.alloc_tensor in: %132, i0, c[74], c[75] dst: %133\n",
      "  call  expand_dims2     in: %6, %133     dst: %134\n",
      "  call  shape_func12     in: %7           dst: %135\n",
      "  call  vm.builtin.load_shape in: %7, c[76]    dst: %136\n",
      "  call  vm.builtin.alloc_storage in: %vm, %136, i0, c[77] dst: %137\n",
      "  call  shape_func11     in: %7           dst: %138\n",
      "  call  vm.builtin.load_shape in: %7, c[78]    dst: %139\n",
      "  call  shape_func11     in: %7           dst: %140\n",
      "  call  vm.builtin.load_shape in: %7, c[79]    dst: %141\n",
      "  call  vm.builtin.alloc_tensor in: %137, i0, %139, c[80] dst: %142\n",
      "  call  shape_func11     in: %7           dst: %143\n",
      "  call  vm.builtin.load_shape in: %7, c[81]    dst: %144\n",
      "  call  add2             in: %126, %133, %142 dst: %145\n",
      "  call  shape_func11     in: %7           dst: %146\n",
      "  call  vm.builtin.load_shape in: %7, c[82]    dst: %147\n",
      "  call  shape_func13     in: %7           dst: %148\n",
      "  call  vm.builtin.load_shape in: %7, c[83]    dst: %149\n",
      "  call  vm.builtin.alloc_storage in: %vm, %149, i0, c[84] dst: %150\n",
      "  call  shape_func11     in: %7           dst: %151\n",
      "  call  vm.builtin.load_shape in: %7, c[85]    dst: %152\n",
      "  call  shape_func11     in: %7           dst: %153\n",
      "  call  vm.builtin.load_shape in: %7, c[86]    dst: %154\n",
      "  call  vm.builtin.alloc_tensor in: %150, i0, %152, c[87] dst: %155\n",
      "  call  shape_func11     in: %7           dst: %156\n",
      "  call  vm.builtin.load_shape in: %7, c[88]    dst: %157\n",
      "  call  softmax          in: %142, %155   dst: %158\n",
      "  call  shape_func11     in: %7           dst: %159\n",
      "  call  vm.builtin.load_shape in: %7, c[89]    dst: %160\n",
      "  call  shape_func11     in: %7           dst: %161\n",
      "  call  vm.builtin.load_shape in: %7, c[90]    dst: %162\n",
      "  ret   ret %155\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get params and input for the module\n",
    "batch_size = 2\n",
    "shape = (batch_size, *dshape)\n",
    "data = tvm.nd.array(np.random.rand(*shape).astype(np.float32))\n",
    "params = list(params_dict.values())\n",
    "\n",
    "# Build the Relax IRModule\n",
    "ex = relax.vm.build(relax_mod, target)\n",
    "\n",
    "print(ex.stats())\n",
    "print(ex.as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a89a94",
   "metadata": {},
   "source": [
    "## Execute Relax IR module\n",
    "\n",
    "Let's run both models and compare the results so we can be certain that the conversion has preserved the semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd1d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 784), 'float32'), ('TENSOR', (128, 784), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 128), 'float32'), ('TENSOR', (64, 128), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=llvm -keys=cpu -link-params=0, workload=('dense_pack.x86', ('TENSOR', ({any_dim|any_dim>=0}, 64), 'float32'), ('TENSOR', (10, 64), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "/home/prakalp/repos/relax/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  \"target_host parameter is going to be deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "res = vm[\"main\"](data, *params)\n",
    "\n",
    "# check correctness by comparing with relay result\n",
    "exe = relay.vm.compile(relay_mod, target)\n",
    "relay_vm = vm_rt.VirtualMachine(exe, tvm.cpu())\n",
    "inputs = [data] + params\n",
    "expected_output = relay_vm.run(*inputs)\n",
    "tvm.testing.assert_allclose(res.numpy(), expected_output.numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc43b24afbafa24450c433caa80e216064e86ab300af4b30503108fd4e5dd3a0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
