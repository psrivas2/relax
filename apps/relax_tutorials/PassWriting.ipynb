{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fccf4446-7343-42c6-9aed-8ffc22178853",
            "metadata": {},
            "source": [
                "# Adding a Compiler Pass to Relax\n",
                "\n",
                "As with Relay, the primary means of implementing optimizations and analyses in Relax is via compiler passes. This tutorial gives an overview of different ways to traverse as well as transform the Relax AST. In addition to that, with the help of an example, the tutorial will introduce writing Relax passes that analyze and transform various components of Relax AST."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f96d4ea9",
            "metadata": {},
            "source": [
                "## AST Traversal\n",
                "\n",
                "In a traditional compilation process, a source program is represented as text and is then parsed into a tree of grammar constructs, which is called an [abstract syntax tree (AST)](https://en.wikipedia.org/wiki/Abstract_syntax_tree), that can be processed more easily than the source program. (In Relax, we use a decorator to parse Python code and produce a Relax AST, but the principle is the same.) Compiler passes operate on the AST by traversing the tree of grammar constructs — hence we say that they make a *pass* over the AST — and either:\n",
                "\n",
                "1. modify the AST (for example, inline functions, unroll loops, etc.), thus implementing a *program transformation*, or\n",
                "2. collect information about AST **without** modifying it (for example, number of loops in the program), thus implementing a *program analysis.*\n",
                "\n",
                "Analyses can be helpful for implementing program transformations by checking that assumptions about the input program have been met. This tutorial will outline the steps for implementing analyses and optimizations in Relax and, in the process, explain the mechanisms for doing so.\n",
                "\n",
                "### Traversing the AST: Functors\n",
                "\n",
                "Abstract syntax trees (ASTs) are, by their nature, recursive: Expressions in a program contain subexpressions, which may in turn contain further subexpressions. Hence, AST traversals must also be done in a recursive process and in a way that allows for customizing the order of the traversal and handling for different grammar constructs.\n",
                "\n",
                "In compilers that are implemented in object-oriented programming languages, this is generally accomplished using the [visitor pattern](https://en.wikipedia.org/wiki/Visitor_pattern), in which the visitor (the parent class) examines the root of the AST using its `visit` method and passes the AST root to a method that operates on that particular grammar construct. For example, if the root of the AST is an `IfNode`, the visitor will dispatch the `visit_if` method. The individual visitor methods can perform whatever operations are necessary and, crucially, also call the parent class's `visit` method to process any subexpressions recursively without worrying about how to dispatch them, since the recursive call to `visit` handles that.\n",
                "\n",
                "For Relax’s AST, the most general form of the visitor pattern is called an `ExprFunctor`, which is a base class for any compiler pass. Implementations are provided in both [C++](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/include/tvm/relax/expr_functor.h#L68-L139) and [Python](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/python/tvm/relax/expr_functor.py#L36-L119). In the C++ implementation, `VisitExpr` is the entry-point method (taking any `Expr` node and dispatching accordingly) and there are overrides of `VisitExpr_` for each grammar construct to handle the different cases. Since Python does not support overloading by argument types, the Python version uses `visit_expr` as the entry point and `visit_{name}_` methods for each grammar construct. (Note: One difference between the versions is that the C++ version also permits auxiliary arguments to be passed in addition to the input AST, though this can be accomplished in Python by wrapping the visitor methods.)\n",
                "\n",
                "Note that the entry-point method can be overridden to include logic that fires before every expression node. This is a commonly used pattern in both the Relay and Relax codebases, with an example given below. (The same can be accomplished in Python using `super`.)\n",
                "\n",
                "```cpp\n",
                "void PrintVisitor::VisitExpr(const Expr& expr) {\n",
                "  // fires for every expression type\n",
                "  std::cout << \"Here\" << std::endl;\n",
                "  // uses the base class's unmodified entry-point method to dispatch by node type\n",
                "  ExprFunctor::VisitExpr(expr);\n",
                "}\n",
                "```\n",
                "\n",
                "While `ExprFunctor`s are general enough to allow for implementing any compiler pass using them, they leave all of of the individual visitor cases undefined, requiring the user to fill them in. Passes typically only have a few node types of interest that affect them, so using a functor would require filling in implementations for many node types that likely will not require special processing. For common cases, we provide child classes of `ExprFunctor` that have sensible defaults and thus require overriding fewer cases. We discuss them below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2675afcf",
            "metadata": {},
            "source": [
                "\n",
                "### Implementing Analyses: Visitors\n",
                "\n",
                "An `ExprVisitor` is an `ExprFunctor` that traverses an AST and, by default, does not modify the AST. This it is used as a parent class for writing analysis passes. The base `ExprVisitor` includes a default implementation for each case, namely visiting each subexpression of the current AST node in order. There are, again, implementations in both [C++](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/src/relax/ir/expr_functor.cc#L36-L197) and [Python](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/python/tvm/relax/expr_functor.py#L122-L261). \n",
                "\n",
                "For example, here is the visitor case for `IfNode`:\n",
                "\n",
                "```cpp\n",
                "void ExprVisitor::VisitExpr_(const IfNode* op) {\n",
                "  this->VisitSpan(op->span);\n",
                "  this->VisitExpr(op->cond);\n",
                "  this->VisitExpr(op->true_branch);\n",
                "  this->VisitExpr(op->false_branch);\n",
                "}\n",
                "```\n",
                "\n",
                "(`Span`s are used for storing metadata for error messages; they are not AST nodes *per se*.)\n",
                "\n",
                "Even though the individual dispatch methods in `ExprVisitor` do not modify the AST, `ExprVisitor` is nevertheless very useful for implementing analyses. This can be accomplished by keeping some state in the `ExprVisitor` that tracks whatever value the analysis is meant to return, modifying that state in the individual cases, and finally returning the stored state. The basic workflow can be summarized as follows:\n",
                "\n",
                "```python\n",
                "# wrapper over the visitor logic; \n",
                "# it may also make sense to implement this as a method on the visitor,\n",
                "# especially if there may be a need to call it recursively\n",
                "def perform_analysis(program: Expr) -> ReturnType:\n",
                "  visitor = MyProgramAnalysis()\n",
                "  visitor.visit_expr(program)\n",
                "  return visitor.get_state()\n",
                "\n",
                "class MyProgramAnalysis(ExprVisitor):\n",
                "  # initialization logic, etc.\n",
                "\n",
                "  def visit_tuple(self, tuple_expr: Tuple) -> None:\n",
                "    for expr in tuple_expr.fields:\n",
                "      if self.some_condition_is_met(expr):\n",
                "        self.update_state(expr)\n",
                "        # can decide whether or not you need to visit the subexpressions\n",
                "        self.visit_expr(expr)\n",
                "      \n",
                "  # etc., define cases as appropriate\n",
                "\n",
                "  def visit_if(self, if_expr: If) -> None:\n",
                "    if self.another_condition_is_met(if_expr):\n",
                "      self.update_state(if_expr)\n",
                "    # can use the default visitor implementation to dispatch \n",
                "    # if that is the desired behavior\n",
                "    super().visit_if(if_expr)\n",
                "```\n",
                "\n",
                "For a real example, we may consider Relax’s [well-formedness analysis](https://github.com/tlc-pack/relax/blob/relax/src/relax/analysis/well_formed.cc), which checks that the input program is in [A-normal form](https://en.wikipedia.org/wiki/A-normal_form) (further discussed below), that all fields of a shape expression are integer-typed, and that all variables have exactly one definition (so there are no variables used that aren’t defined and no variable defined more than once).\n",
                "\n",
                "The well-formedness checker keeps a Boolean field called `well_formed` that is true if the program meets the criteria for well-formedness and false otherwise. The visitor is invoked from a wrapper method called `WellFormed` that initializes the visitor, applies it to every function in the `IRModule`, and returns the final value of `well_formed`:\n",
                "\n",
                "```cpp\n",
                "bool WellFormed(const IRModule& m, Optional<DiagnosticContext> diag_ctx) {\n",
                "  WellFormedChecker well_formed_checker = WellFormedChecker(diag_ctx);\n",
                "  for (const auto& it : m->functions) {\n",
                "    // register GlobalVar in the IRModule first\n",
                "    well_formed_checker.RegisterGlobalVar(it.first);\n",
                "  }\n",
                "\n",
                "  for (const auto& it : m->functions) {\n",
                "    // visit relax.Function\n",
                "    if (auto* n = it.second.as<FunctionNode>()) {\n",
                "      Function func = GetRef<Function>(n);\n",
                "      well_formed_checker.VisitExpr(func);\n",
                "    }\n",
                "  }\n",
                "\n",
                "  return well_formed_checker.well_formed;\n",
                "}\n",
                "```\n",
                "\n",
                "Here is the well-formedness  checker’s logic for handling bound variables (e.g., parameters to functions or variables in assignments):\n",
                "\n",
                "```cpp\n",
                "void VisitVarDef_(const VarNode* var) {\n",
                "    Var gv = GetRef<Var>(var);\n",
                "    if (var_set_.count(gv) == 1) {\n",
                "      Malformed(Diagnostic::Error(var->span)\n",
                "                << \"Var \" << gv->name_hint() << \" is defined more than once.\");\n",
                "    }\n",
                "    // register Var\n",
                "    var_set_.insert(gv);\n",
                "}\n",
                "```\n",
                "\n",
                "First, it checks that the variable being visited does not already have a definition associated with it (giving an error if it does). Next, it adds the variable to its set of variables with definitions, which it can use to validate any future variable definitions encountered during the AST traversal.\n",
                "\n",
                "Meanwhile, the case for `IfNode` does some recursive visits and has to manage some of the persistent state:\n",
                "\n",
                "```cpp\n",
                "void VisitExpr_(const IfNode* op) {\n",
                "    this->VisitExpr(op->cond);\n",
                "    std::unordered_set<Var, ObjectPtrHash, ObjectPtrEqual> previous_var_set_ = var_set_;\n",
                "    std::unordered_set<tir::Var, ObjectPtrHash, ObjectPtrEqual> previous_symbolic_var_set_ =\n",
                "        prim_expr_visitor_.symbolic_var_set_;\n",
                "    this->VisitBody(op->true_branch);\n",
                "    var_set_ = previous_var_set_;\n",
                "    prim_expr_visitor_.symbolic_var_set_ = previous_symbolic_var_set_;\n",
                "    this->VisitBody(op->false_branch);\n",
                "    var_set_ = previous_var_set_;\n",
                "    prim_expr_visitor_.symbolic_var_set_ = previous_symbolic_var_set_;\n",
                "}\n",
                "```\n",
                "\n",
                "Before visiting the true branch and false branches of the `IfNode`, the visitor stores the previous sets of definitions because the branches are [lexically scoped](https://en.wikipedia.org/wiki/Scope_(computer_science)#Lexical_scope_vs._dynamic_scope), so definitions in the branches are not visible outside the branches. This is an example of how the logic in a compiler pass may need to account for the semantics of Relax.\n",
                "\n",
                "Many analyses in Relax are implemented using a function called  `PostOrderVisit`, which simply recurses down an AST and applies a user-provided function on each AST node *after* completing any recursive visits for that node (hence “post-order”). Its logic is very simple and allows these analyses to be implemented very compactly:\n",
                "\n",
                "```cpp\n",
                "class ExprApplyVisit : public ExprVisitor {\n",
                " public:\n",
                "  explicit ExprApplyVisit(std::function<void(const Expr&)> f) : f_(f) {}\n",
                "\n",
                "  void VisitExpr(const Expr& e) final {\n",
                "    ExprVisitor::VisitExpr(e);\n",
                "    f_(e);\n",
                "  }\n",
                "\n",
                " private:\n",
                "  std::function<void(const Expr&)> f_;\n",
                "};\n",
                "\n",
                "void PostOrderVisit(const Expr& e, std::function<void(const Expr&)> fvisit) {\n",
                "  ExprApplyVisit(fvisit).VisitExpr(e);\n",
                "}\n",
                "```\n",
                "\n",
                "### Implementing Transformations: (Base) Mutators\n",
                "\n",
                "`ExprVisitor`s are useful when the goal is to examine an input program and draw some conclusion about it, but implementing compiler optimizations generally requires making changes to programs. `ExprMutator`s are used for the latter purpose, but they do not actually change an AST in-place; rather, they traverse the AST and produce a new AST that may incorporate changes. In fact, the default implementation of an `ExprMutator` simply traverses an AST and returns the very same AST—this allows users to override individual cases to have the mutator return a different program, with the helpful default that any cases left unchanged will simply preserve those parts of the program. This generally presents the abstraction of making changes to a program when what is really happening is that a new program is being constructed. See the implementations in [C++](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/src/relax/ir/expr_functor.cc#L223-L665) and [Python](https://github.com/tlc-pack/relax/blob/6495823859cdcbe87899b31de1b82a824e08c4f3/python/tvm/relax/expr_functor.py#L264-L714).\n",
                "\n",
                "For example, here is the default implementation for the tuple node case:\n",
                "\n",
                "```cpp\n",
                "Expr ExprMutatorBase::VisitExpr_(const TupleNode* op) {\n",
                "  bool unchanged = true;\n",
                "  tvm::Array<Expr> fields;\n",
                "  for (Expr field : op->fields) {\n",
                "    Expr new_field = this->VisitExpr(field);\n",
                "    fields.push_back(new_field);\n",
                "    unchanged &= new_field.same_as(field);\n",
                "  }\n",
                "\n",
                "  if (unchanged) {\n",
                "    return GetRef<Expr>(op);\n",
                "  } else {\n",
                "    Expr new_tuple = Tuple(fields, op->span);\n",
                "    return new_tuple;\n",
                "  }\n",
                "}\n",
                "```\n",
                "\n",
                "Notice that the method simply returns the original node if the result of visiting all the subexpressions is the same as in the original expression; this avoids allocating new AST nodes for no reason. (Also, since Relax AST nodes are immutable, this is safe.) Thus, if a visit to one of the fields of this tuple results in a change being made to the AST, the result will be a new tuple node that includes the changed field.\n",
                "\n",
                "The basic workflow of implementing a program transformation using the base mutator might look like this:\n",
                "\n",
                "```python\n",
                "class MyProgramTransformation(ExprMutatorBase):\n",
                "  def visit_if_(self, if_expr):\n",
                "    # e.g., if it matches some pattern\n",
                "    if some_condition_is_met(if_expr):\n",
                "      # essentially replaces the encountered expression with some other expression\n",
                "      # (in reality, a new program is built featuring the new subexpression)\n",
                "      return my_new_node()\n",
                "    super().visit_expr(if_expr)\n",
                "\n",
                "  # any cases not overridden will preserve the program as it was\n",
                "```\n",
                "\n",
                "### Using A-Normal Form in `ExprMutator`\n",
                "\n",
                "The class shown above was `ExprMutatorBase` rather than `ExprMutator`. That is because the `ExprMutator` used to implement program transformations in Relax does not directly traverse arbitrary ASTs like in the example given, but instead expects ASTs to first be *normalized*, namely put into *[administrative normal form](https://en.wikipedia.org/wiki/A-normal_form)*, abbreviated “A-normal form” or “ANF.”\n",
                "\n",
                "In ANF, expressions are allowed to have only *atomic* *expressions* (constants, function definitions, or variables) for subexpressions, meaning that complex expressions like call nodes cannot contain other complex expressions as subexpressions—the only way to pass their results to other complex expressions is to bind them to variables. For example, `f(g(x))` and `if f(x) then 1 else 0` are not permitted in ANF, while  `a = g(x); f(a)` and `a = f(x); if a then 1 else 0` are.\n",
                "\n",
                "As described in [this page](https://matt.might.net/articles/a-normalization/) by Matt Might, one advantage of ANF for compilers is that it transforms a a deeply nested program into one where the order of operations is spelled out very clearly in a series of bindings. It is often useful in optimizations to reason directly about execution order and, as Might also observes, such bindings are easy to lower to assembly instructions. \n",
                "\n",
                "In Relax’s `ExprMutator`, the use of ANF has several advantages for writing passes:\n",
                "\n",
                "1. It generally simplifies the handling of complex expressions by guaranteeing that subexpressions are variables or constants, thus reducing the amount of analysis needed to conclude that certain program transformations are safe. For example, consider the case of a call node: If the arguments to the call could be general expressions, it is possible, in principle, that one of the subexpressions is a call to a `PackedFunc` that has side effects. Thus, it would change the program’s semantics to either eliminate the call or reorder the arguments. In ANF, however, the arguments are guaranteed to be atomic expressions and thus cannot have any side effects.\n",
                "2. In ANF, similarly to [single static assignment (SSA)](https://en.wikipedia.org/wiki/Static_single-assignment_form), complex nested expressions are instead transformed into a series of bindings. This way, the subexpressions can be addressed individually, which is often convenient for writing passes. (Indeed, the `ExprMutator` implementations include helper methods for tracking which variables are in scope and what their definitions are, allowing pass implementations to easily query for them.) Additionally, there is less need for passes to reason about the order of operations within a nested expression—all is determined by the order of the bindings, and it is easy to introduce new bindings and reorder them as needed.\n",
                "3. It is common for Relax passes to add bindings to the current scope (a binding block). In a deeply nested expression, it can become difficult to reason about the order in which bindings will be added and whether there are any dependencies between these. With ANF, this is never an issue because the amount of nesting is limited.\n",
                "\n",
                "[The `Normalize` pass in Relax](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/normalize.cc) transforms an arbitrary Relax AST into ANF.\n",
                "\n",
                "For a real example, `ExprMutator` was used to implement a [constant-folding pass in Relax](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fold_constant.cc) rather compactly. For variables that are bound to constants, the mutator simply replaces the variables with those constants:\n",
                "\n",
                "```python\n",
                "Expr VisitExpr_(const VarNode* op) final {\n",
                "    Optional<Expr> opt = LookupBinding(GetRef<Var>(op));\n",
                "    // `as` check checks if opt is not null and is instance of constant\n",
                "    if (opt.as<relax::ConstantNode>()) {\n",
                "      return opt.value();\n",
                "    }\n",
                "    return ExprMutator::VisitExpr_(op);\n",
                "}\n",
                "```\n",
                "\n",
                "The only complex case is handling calls to TIR functions: If there is a call to a TIR function and all the arguments are constants, then the TIR call itself can simply be evaluated at compile time.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "967791fb",
            "metadata": {},
            "source": [
                "## Granularity\n",
                "### Function\n",
                "### Dataflow Block\n",
                "### Module"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24f4b7ca",
            "metadata": {},
            "source": [
                "## Example: Conv-Conv Fusion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "e7af6b36",
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "import tvm\n",
                "from tvm import relax, topi\n",
                "from tvm.script import relax as R, tir as T\n",
                "from tvm.ir.transform import module_pass\n",
                "from tvm.relax import ExprMutator\n",
                "from tvm.ir.module import IRModule\n",
                "from tvm import tir, relax"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Unfused Convolution Module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "6c20f945",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">conv2d</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;conv2d&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;my_op_kind&quot;</span>: <span style=\"color: #BA2121\">&quot;convolution&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], rxplaceholder_1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (x, w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (lv, w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "bb = relax.BlockBuilder()\n",
                "\n",
                "# Main function\n",
                "def build_model(x, w1, w2):\n",
                "    \n",
                "    with bb.function(\"main\", [x, w1, w2]):\n",
                "        with bb.dataflow():\n",
                "            lv1 = bb.emit_te(topi.nn.conv2d, x, w1, strides=1, padding=1, dilation=1)\n",
                "            gv = bb.emit_output(bb.emit_te(topi.nn.conv2d, lv1, w2, strides=1, padding=1, dilation=1))\n",
                "        bb.emit_func_output(gv)\n",
                "    \n",
                "    return bb.get()\n",
                "\n",
                "tensor_type = relax.DynTensorType(4, \"float32\")\n",
                "x = relax.Var(\"x\", (1, 16, 64, 64), tensor_type)\n",
                "w1 = relax.Var(\"w1\", (16, 16, 3, 3), tensor_type)\n",
                "w2 = relax.Var(\"w2\", (16, 16, 3, 3), tensor_type)\n",
                "\n",
                "mod = build_model(x, w1, w2)\n",
                "mod[\"conv2d\"] = mod[\"conv2d\"].with_attr(\"my_op_kind\", \"convolution\")\n",
                "mod.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "08c3769e",
            "metadata": {},
            "outputs": [],
            "source": [
                "class FuseTwoConvMutator(ExprMutator):\n",
                "\n",
                "    def __init__(self, mod: IRModule) -> None:\n",
                "        super().__init__(mod)\n",
                "        self.mod_ = mod\n",
                "\n",
                "    def is_convolution(self, call_node):\n",
                "        call_tir_op = tvm.ir.Op.get(\"relax.call_tir\")\n",
                "        if call_node.op != call_tir_op:\n",
                "            return False\n",
                "        global_var = call_node.args[0]\n",
                "        tir_func = self.mod_[global_var]\n",
                "        if tir_func.attrs[\"my_op_kind\"] != \"convolution\":\n",
                "            return False\n",
                "        return tir_func\n",
                "    \n",
                "    def pattern_match(self, call_node):\n",
                "        first_conv_tir = self.is_convolution(call_node)\n",
                "        if not first_conv_tir:\n",
                "            return None\n",
                "        operands = call_node.args[1]\n",
                "        input_tensor = operands[0]\n",
                "        value = self.lookup_binding(input_tensor)\n",
                "        if not value:\n",
                "            return None\n",
                "        if not isinstance(value, relax.Call):\n",
                "            return None\n",
                "        second_conv_tir = self.is_convolution(value)\n",
                "        if not second_conv_tir:\n",
                "            return None\n",
                "        return [value, call_node]\n",
                "\n",
                "    def transform(self) -> IRModule:\n",
                "        for global_var, func in self.mod_.functions.items():\n",
                "            if isinstance(func, relax.Function):\n",
                "                func = self.visit_expr(func)\n",
                "                self.builder_.update_func(global_var, func)\n",
                "        return self.builder_.get()\n",
                "    \n",
                "    def visit_call_(self, call_node: relax.Call) -> relax.Call:\n",
                "        conv_calls = self.pattern_match(call_node)\n",
                "        if not conv_calls:\n",
                "            return call_node\n",
                "        # Build fused op\n",
                "        # construct a subgraph\n",
                "        # R.parser.pretty_print(conv_call[0])\n",
                "        args = [conv_calls[0].args[1][0], conv_calls[0].args[1][1], conv_calls[1].args[1][1]]\n",
                "        x = relax.Var(\"x\", args[0].shape_, args[0]._checked_type_)\n",
                "        w1 = relax.Var(\"w1\", args[1].shape_, args[1]._checked_type_)\n",
                "        w2 = relax.Var(\"w2\", args[2].shape_, args[2]._checked_type_)\n",
                "        conv2d = call_node.args[0]\n",
                "\n",
                "        lv0 = relax.DataflowVar(\"lv0\", conv_calls[0].shape_, conv_calls[0]._checked_type_)\n",
                "        conv_1 = relax.call_tir(conv2d, [x, w1], conv_calls[0].shape_, dtype=\"float32\")\n",
                "        b0 = relax.VarBinding(lv0, conv_1)\n",
                "        gv = relax.Var(\"conv_2\", conv_calls[1].shape_, conv_calls[1]._checked_type_)\n",
                "\n",
                "        b1 = relax.VarBinding(gv, relax.call_tir(conv2d, [lv0, w2], conv_calls[1].shape_, dtype=\"float32\"))\n",
                "        bindings = [b0, b1]\n",
                "        blocks = [relax.DataflowBlock(bindings)]\n",
                "        seq_expr = relax.SeqExpr(blocks, blocks[-1].bindings[-1].var)\n",
                "        ret_type = conv_calls[1]._checked_type_\n",
                "\n",
                "\n",
                "        func_name = \"fused_conv_conv\"\n",
                "        fused_conv_conv = relax.Function([x, w1, w2], seq_expr, ret_type).with_attr(\"global_symbol\", func_name).with_attr(\"Primitive\", 1)\n",
                "        normalized = self.builder_.normalize(fused_conv_conv)\n",
                "        global_var = self.builder_.add_func(normalized, \"fused_conv_conv\")\n",
                "        # self.builder_.add_func(conv2d, \"conv2d\")\n",
                "\n",
                "        # construct a call to the subgraph\n",
                "        fused_conv_call = relax.Call(global_var, [args[0], args[1], args[2]], None, None)\n",
                "        return fused_conv_call\n",
                "        \n",
                "\n",
                "@module_pass(opt_level=2, name=\"fuse_two_conv\")\n",
                "class FuseTwoConv:\n",
                "    \"\"\"The wrapper for the FuseTwoConv pass.\"\"\"\n",
                "\n",
                "    def transform_module(self, mod, ctx):\n",
                "        return FuseTwoConvMutator(mod).transform()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "0d3b5f0f",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">conv2d</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;conv2d&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;my_op_kind&quot;</span>: <span style=\"color: #BA2121\">&quot;convolution&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], rxplaceholder_1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (x, w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            lv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> fused_conv_conv(x, w1, w2)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_conv_conv</span>(x1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w11: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w21: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;fused_conv_conv&quot;</span>, <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (x1, w11), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            conv_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (lv0, w21), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(conv_2)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> conv_2\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "after = FuseTwoConv()(mod)\n",
                "if not relax.analysis.well_formed(after):\n",
                "    print(\"NOT WELL FORMED\")\n",
                "after.show(\"dark\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "adfda8da",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">conv2d</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;conv2d&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;my_op_kind&quot;</span>: <span style=\"color: #BA2121\">&quot;convolution&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], rxplaceholder_1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_conv_conv</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], w1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], w2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;fused_conv_conv&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        conv2d_nchw_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        pad_temp_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], w1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw_1[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w1[ff, rc, ry, rx]\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp_1&quot;</span>):\n",
                            "                i0_2, i1_2, i2_2, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp_1[i0_2, i1_2, i2_2, i3_2])\n",
                            "                pad_temp_1[i0_2, i1_2, i2_2, i3_2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_2 <span style=\"color: #008000; font-weight: bold\">and</span> i2_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_2 <span style=\"color: #008000; font-weight: bold\">and</span> i3_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0_3, i1_3, i2_3, i3_3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw_1&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0_3, i1_3, i2_3, i3_3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], w2[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w2[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (x, w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(fused_conv_conv, (x, w1, w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "after_tir_fuse = relax.transform.FuseTIR()(after)\n",
                "if not relax.analysis.well_formed(after_tir_fuse):\n",
                "    print(\"NOT WELL FORMED\")\n",
                "after_tir_fuse.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b545d0e4",
            "metadata": {},
            "source": [
                "## Play With Fused TIR Schedule"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "fc43b24afbafa24450c433caa80e216064e86ab300af4b30503108fd4e5dd3a0"
        },
        "kernelspec": {
            "display_name": "Python 3.7.11 ('py37')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
