{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fccf4446-7343-42c6-9aed-8ffc22178853",
            "metadata": {},
            "source": [
                "# Adding a Compiler Pass to Relax\n",
                "\n",
                "As with Relay, the primary means of implementing optimizations and analyses in Relax is via compiler passes. This tutorial gives an overview of different ways to traverse as well as transform the Relax AST. In addition to that, with the help of an example, the tutorial will introduce writing Relax passes that analyze and transform various components of Relax AST."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f96d4ea9",
            "metadata": {},
            "source": [
                "## AST Traversal\n",
                "\n",
                "In a traditional compilation process, a source program is represented as text and is then parsed into a tree of grammar constructs, which is called an [abstract syntax tree (AST)](https://en.wikipedia.org/wiki/Abstract_syntax_tree), that can be processed more easily than the source program. (In Relax, we use a decorator to parse Python code and produce a Relax AST, but the principle is the same.) Compiler passes operate on the AST by traversing the tree of grammar constructs — hence we say that they make a *pass* over the AST — and either:\n",
                "\n",
                "1. modify the AST (for example, inline functions, unroll loops, etc.), thus implementing a *program transformation*, or\n",
                "2. collect information about AST **without** modifying it (for example, number of loops in the program), thus implementing a *program analysis.*\n",
                "\n",
                "Analyses can be helpful for implementing program transformations by checking that assumptions about the input program have been met. This tutorial will outline the steps for implementing analyses and optimizations in Relax and, in the process, explain the mechanisms for doing so.\n",
                "\n",
                "### Traversing the AST: Functors\n",
                "\n",
                "Abstract syntax trees (ASTs) are, by their nature, recursive: Expressions in a program contain subexpressions, which may in turn contain further subexpressions. Hence, AST traversals must also be done in a recursive process and in a way that allows for customizing the order of the traversal and handling for different grammar constructs.\n",
                "\n",
                "In compilers that are implemented in object-oriented programming languages, this is generally accomplished using the [visitor pattern](https://en.wikipedia.org/wiki/Visitor_pattern), in which the visitor (the parent class) examines the root of the AST using its `visit` method and passes the AST root to a method that operates on that particular grammar construct. For example, if the root of the AST is an `IfNode`, the visitor will dispatch the `visit_if` method. The individual visitor methods can perform whatever operations are necessary and, crucially, also call the parent class's `visit` method to process any subexpressions recursively without worrying about how to dispatch them, since the recursive call to `visit` handles that.\n",
                "\n",
                "For Relax’s AST, the most general form of the visitor pattern is called an `ExprFunctor`, which is a base class for any compiler pass. Implementations are provided in both [C++](https://github.com/tlc-pack/relax/blob/relax/include/tvm/relax/expr_functor.h) and [Python](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/expr_functor.py). In the C++ implementation, `VisitExpr` is the entry-point method (taking any `Expr` node and dispatching accordingly) and there are overrides of `VisitExpr_` for each grammar construct to handle the different cases. Since Python does not support overloading by argument types, the Python version uses `visit_expr` as the entry point and `visit_{name}_` methods for each grammar construct. (Note: One difference between the versions is that the C++ version also permits auxiliary arguments to be passed in addition to the input AST, though this can be accomplished in Python by wrapping the visitor methods.)\n",
                "\n",
                "Note that the entry-point method can be overridden to include logic that fires before every expression node. This is a commonly used pattern in both the Relay and Relax codebases, with an example given below. (The same can be accomplished in Python using `super`.)\n",
                "\n",
                "```cpp\n",
                "void PrintVisitor::VisitExpr(const Expr& expr) {\n",
                "  // fires for every expression type\n",
                "  std::cout << \"Here\" << std::endl;\n",
                "  // uses the base class's unmodified entry-point method to dispatch by node type\n",
                "  ExprFunctor::VisitExpr(expr);\n",
                "}\n",
                "```\n",
                "\n",
                "While `ExprFunctor`s are general enough to allow for implementing any compiler pass using them, they leave all of of the individual visitor cases undefined, requiring the user to fill them in. Passes typically only have a few node types of interest that affect them, so using a functor would require filling in implementations for many node types that likely will not require special processing. For common cases, we provide child classes of `ExprFunctor` that have sensible defaults and thus require overriding fewer cases. We discuss them below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2675afcf",
            "metadata": {},
            "source": [
                "\n",
                "### Implementing Analyses: Visitors\n",
                "\n",
                "An `ExprVisitor` is an `ExprFunctor` that traverses an AST and, by default, does not modify the AST. This it is used as a parent class for writing analysis passes. The base `ExprVisitor` includes a default implementation for each case, namely visiting each subexpression of the current AST node in order. There are, again, implementations in both [C++](https://github.com/tlc-pack/relax/blob/relax/include/tvm/relax/expr_functor.h#L186) and [Python](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/expr_functor.py#L351) as `PyExprVisitor`. \n",
                "\n",
                "For example, here is the visitor case for `IfNode`:\n",
                "\n",
                "```cpp\n",
                "void ExprVisitor::VisitExpr_(const IfNode* op) {\n",
                "  this->VisitSpan(op->span);\n",
                "  this->VisitExpr(op->cond);\n",
                "  this->VisitExpr(op->true_branch);\n",
                "  this->VisitExpr(op->false_branch);\n",
                "}\n",
                "```\n",
                "\n",
                "(`Span`s are used for storing metadata for error messages; they are not AST nodes *per se*.)\n",
                "\n",
                "Even though the individual dispatch methods in `ExprVisitor` do not modify the AST, `ExprVisitor` is nevertheless very useful for implementing analyses. This can be accomplished by keeping some state in the `ExprVisitor` that tracks whatever value the analysis is meant to return, modifying that state in the individual cases, and finally returning the stored state. The basic workflow can be summarized as follows:\n",
                "\n",
                "```python\n",
                "# wrapper over the visitor logic; \n",
                "# it may also make sense to implement this as a method on the visitor,\n",
                "# especially if there may be a need to call it recursively\n",
                "def perform_analysis(program: Expr) -> ReturnType:\n",
                "  visitor = MyProgramAnalysis()\n",
                "  visitor.visit_expr(program)\n",
                "  return visitor.get_state()\n",
                "\n",
                "@relax.expr_functor.visitor\n",
                "class MyProgramAnalysis(PyExprVisitor):\n",
                "  # initialization logic, etc.\n",
                "\n",
                "  def visit_tuple_(self, tuple_expr: Tuple) -> None:\n",
                "    for expr in tuple_expr.fields:\n",
                "      if self.some_condition_is_met(expr):\n",
                "        self.update_state(expr)\n",
                "        # can decide whether or not you need to visit the subexpressions\n",
                "        self.visit_expr(expr)\n",
                "      \n",
                "  # etc., define cases as appropriate\n",
                "\n",
                "  def visit_if_(self, if_expr: If) -> None:\n",
                "    if self.another_condition_is_met(if_expr):\n",
                "      self.update_state(if_expr)\n",
                "    # can use the default visitor implementation to dispatch \n",
                "    # if that is the desired behavior\n",
                "    super().visit_if_(if_expr)\n",
                "```\n",
                "\n",
                "For a real example, we may consider Relax’s [well-formedness analysis](https://github.com/tlc-pack/relax/blob/relax/src/relax/analysis/well_formed.cc), which checks that the input program is in [A-normal form](https://en.wikipedia.org/wiki/A-normal_form) (further discussed below), that all fields of a shape expression are integer-typed, and that all variables have exactly one definition (so there are no variables used that aren’t defined and no variable defined more than once).\n",
                "\n",
                "The well-formedness checker keeps a Boolean field called `well_formed` that is true if the program meets the criteria for well-formedness and false otherwise. The visitor is invoked from a wrapper method called `WellFormed` that initializes the visitor, applies it to every function in the `IRModule`, and returns the final value of `well_formed`:\n",
                "\n",
                "```cpp\n",
                "bool WellFormed(const IRModule& m, Optional<DiagnosticContext> diag_ctx) {\n",
                "  WellFormedChecker well_formed_checker = WellFormedChecker(diag_ctx);\n",
                "  for (const auto& it : m->functions) {\n",
                "    // register GlobalVar in the IRModule first\n",
                "    well_formed_checker.RegisterGlobalVar(it.first);\n",
                "  }\n",
                "\n",
                "  for (const auto& it : m->functions) {\n",
                "    // visit relax.Function\n",
                "    if (auto* n = it.second.as<FunctionNode>()) {\n",
                "      Function func = GetRef<Function>(n);\n",
                "      well_formed_checker.VisitExpr(func);\n",
                "    }\n",
                "  }\n",
                "\n",
                "  return well_formed_checker.well_formed;\n",
                "}\n",
                "```\n",
                "\n",
                "Here is the well-formedness  checker’s logic for handling bound variables (e.g., parameters to functions or variables in assignments):\n",
                "\n",
                "```cpp\n",
                "void VisitVarDef_(const VarNode* var) {\n",
                "    Var gv = GetRef<Var>(var);\n",
                "    if (var_set_.count(gv) == 1) {\n",
                "      Malformed(Diagnostic::Error(var->span)\n",
                "                << \"Var \" << gv->name_hint() << \" is defined more than once.\");\n",
                "    }\n",
                "    // register Var\n",
                "    var_set_.insert(gv);\n",
                "}\n",
                "```\n",
                "\n",
                "First, it checks that the variable being visited does not already have a definition associated with it (giving an error if it does). Next, it adds the variable to its set of variables with definitions, which it can use to validate any future variable definitions encountered during the AST traversal.\n",
                "\n",
                "Meanwhile, the case for `IfNode` does some recursive visits and has to manage some of the persistent state:\n",
                "\n",
                "```cpp\n",
                "void VisitExpr_(const IfNode* op) {\n",
                "    this->VisitExpr(op->cond);\n",
                "    std::unordered_set<Var, ObjectPtrHash, ObjectPtrEqual> previous_var_set_ = var_set_;\n",
                "    std::unordered_set<tir::Var, ObjectPtrHash, ObjectPtrEqual> previous_symbolic_var_set_ =\n",
                "        prim_expr_visitor_.symbolic_var_set_;\n",
                "    this->VisitBody(op->true_branch);\n",
                "    var_set_ = previous_var_set_;\n",
                "    prim_expr_visitor_.symbolic_var_set_ = previous_symbolic_var_set_;\n",
                "    this->VisitBody(op->false_branch);\n",
                "    var_set_ = previous_var_set_;\n",
                "    prim_expr_visitor_.symbolic_var_set_ = previous_symbolic_var_set_;\n",
                "}\n",
                "```\n",
                "\n",
                "Before visiting the true branch and false branches of the `IfNode`, the visitor stores the previous sets of definitions because the branches are [lexically scoped](https://en.wikipedia.org/wiki/Scope_(computer_science)#Lexical_scope_vs._dynamic_scope), so definitions in the branches are not visible outside the branches. This is an example of how the logic in a compiler pass may need to account for the semantics of Relax.\n",
                "\n",
                "Many analyses in Relax are implemented using a function called  `PostOrderVisit`, which simply recurses down an AST and applies a user-provided function on each AST node *after* completing any recursive visits for that node (hence “post-order”). Its logic is very simple and allows these analyses to be implemented very compactly:\n",
                "\n",
                "```cpp\n",
                "class ExprApplyVisit : public ExprVisitor {\n",
                " public:\n",
                "  explicit ExprApplyVisit(std::function<void(const Expr&)> f) : f_(f) {}\n",
                "\n",
                "  void VisitExpr(const Expr& e) final {\n",
                "    ExprVisitor::VisitExpr(e);\n",
                "    f_(e);\n",
                "  }\n",
                "\n",
                " private:\n",
                "  std::function<void(const Expr&)> f_;\n",
                "};\n",
                "\n",
                "void PostOrderVisit(const Expr& e, std::function<void(const Expr&)> fvisit) {\n",
                "  ExprApplyVisit(fvisit).VisitExpr(e);\n",
                "}\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dfd74769",
            "metadata": {},
            "source": [
                "\n",
                "### Implementing Transformations: (Base) Mutators\n",
                "\n",
                "`ExprVisitor`s are useful when the goal is to examine an input program and draw some conclusion about it, but implementing compiler optimizations generally requires making changes to programs. `ExprMutator`s are used for the latter purpose, but they do not actually change an AST in-place; rather, they traverse the AST and produce a new AST that may incorporate changes. In fact, the default implementation of an `ExprMutator` simply traverses an AST and returns the very same AST—this allows users to override individual cases to have the mutator return a different program, with the helpful default that any cases left unchanged will simply preserve those parts of the program. This generally presents the abstraction of making changes to a program when what is really happening is that a new program is being constructed. See the implementations in [C++](https://github.com/tlc-pack/relax/blob/relax/include/tvm/relax/expr_functor.h) and [Python](https://github.com/tlc-pack/relax/blob/relax/python/tvm/relax/expr_functor.py) as `PyExprMutator`.\n",
                "\n",
                "For example, here is the default implementation for the tuple node case:\n",
                "\n",
                "```cpp\n",
                "Expr ExprMutatorBase::VisitExpr_(const TupleNode* op) {\n",
                "  bool unchanged = true;\n",
                "  tvm::Array<Expr> fields;\n",
                "  for (Expr field : op->fields) {\n",
                "    Expr new_field = this->VisitExpr(field);\n",
                "    fields.push_back(new_field);\n",
                "    unchanged &= new_field.same_as(field);\n",
                "  }\n",
                "\n",
                "  if (unchanged) {\n",
                "    return GetRef<Expr>(op);\n",
                "  } else {\n",
                "    Expr new_tuple = Tuple(fields, op->span);\n",
                "    return new_tuple;\n",
                "  }\n",
                "}\n",
                "```\n",
                "\n",
                "Notice that the method simply returns the original node if the result of visiting all the subexpressions is the same as in the original expression; this avoids allocating new AST nodes for no reason. (Also, since Relax AST nodes are immutable, this is safe.) Thus, if a visit to one of the fields of this tuple results in a change being made to the AST, the result will be a new tuple node that includes the changed field.\n",
                "\n",
                "The basic workflow of implementing a program transformation using the base mutator might look like this:\n",
                "\n",
                "```python\n",
                "class MyProgramTransformation(ExprMutatorBase):\n",
                "  def visit_if_(self, if_expr):\n",
                "    # e.g., if it matches some pattern\n",
                "    if some_condition_is_met(if_expr):\n",
                "      # essentially replaces the encountered expression with some other expression\n",
                "      # (in reality, a new program is built featuring the new subexpression)\n",
                "      return my_new_node()\n",
                "    return super().visit_expr(if_expr)\n",
                "\n",
                "  # any cases not overridden will preserve the program as it was\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "783bd47d",
            "metadata": {},
            "source": [
                "\n",
                "### Using A-Normal Form in `ExprMutator`\n",
                "\n",
                "The class shown above was `ExprMutatorBase` rather than `ExprMutator`. That is because the `ExprMutator` used to implement program transformations in Relax does not directly traverse arbitrary ASTs like in the example given, but instead expects ASTs to first be *normalized*, namely put into *[administrative normal form](https://en.wikipedia.org/wiki/A-normal_form)*, abbreviated “A-normal form” or “ANF.”\n",
                "\n",
                "In ANF, expressions are allowed to have only *atomic* *expressions* (constants, function definitions, or variables) for subexpressions, meaning that complex expressions like call nodes cannot contain other complex expressions as subexpressions—the only way to pass their results to other complex expressions is to bind them to variables. For example, `f(g(x))` and `if f(x) then 1 else 0` are not permitted in ANF, while  `a = g(x); f(a)` and `a = f(x); if a then 1 else 0` are.\n",
                "\n",
                "As described in [this page](https://matt.might.net/articles/a-normalization/) by Matt Might, one advantage of ANF for compilers is that it transforms a a deeply nested program into one where the order of operations is spelled out very clearly in a series of bindings. It is often useful in optimizations to reason directly about execution order and, as Might also observes, such bindings are easy to lower to assembly instructions. \n",
                "\n",
                "In Relax’s `ExprMutator`, the use of ANF has several advantages for writing passes:\n",
                "\n",
                "1. It generally simplifies the handling of complex expressions by guaranteeing that subexpressions are variables or constants, thus reducing the amount of analysis needed to conclude that certain program transformations are safe. For example, consider the case of a call node: If the arguments to the call could be general expressions, it is possible, in principle, that one of the subexpressions is a call to a `PackedFunc` that has side effects. Thus, it would change the program’s semantics to either eliminate the call or reorder the arguments. In ANF, however, the arguments are guaranteed to be atomic expressions and thus cannot have any side effects.\n",
                "2. In ANF, similarly to [single static assignment (SSA)](https://en.wikipedia.org/wiki/Static_single-assignment_form), complex nested expressions are instead transformed into a series of bindings. This way, the subexpressions can be addressed individually, which is often convenient for writing passes. (Indeed, the `ExprMutator` implementations include helper methods for tracking which variables are in scope and what their definitions are, allowing pass implementations to easily query for them.) Additionally, there is less need for passes to reason about the order of operations within a nested expression—all is determined by the order of the bindings, and it is easy to introduce new bindings and reorder them as needed.\n",
                "3. It is common for Relax passes to add bindings to the current scope (a binding block). In a deeply nested expression, it can become difficult to reason about the order in which bindings will be added and whether there are any dependencies between these. With ANF, this is never an issue because the amount of nesting is limited.\n",
                "4. The ExprMutator automatically normalizes expressions (via the [ExprNormalizer](https://github.com/tlc-pack/relax/blob/relax/src/relax/ir/block_builder.cc#L39) of its internal block builder `builder_`) and update the `checked_type_` and `shape_` fields of expressions, ensuring that changes made during the pass will be appropriately taken into account.\n",
                "\n",
                "*We expect all Relax passes to have ANF input and ANF output and use ExprMutator; ExprMutatorBase is used in very rare cases, currently only Normalization uses ExprMutatorBase (when the user-written TVMScript is not in ANF and may have nested expressions).*\n",
                "\n",
                "[The `Normalize` pass in Relax](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/normalize.cc) transforms an arbitrary Relax AST into ANF.\n",
                "\n",
                "For a real example, `ExprMutator` was used to implement a [constant-folding pass in Relax](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fold_constant.cc) rather compactly. For variables that are bound to constants, the mutator simply replaces the variables with those constants:\n",
                "\n",
                "```python\n",
                "Expr VisitExpr_(const VarNode* op) final {\n",
                "    Optional<Expr> opt = LookupBinding(GetRef<Var>(op));\n",
                "    // `as` check checks if opt is not null and is instance of constant\n",
                "    if (opt.as<relax::ConstantNode>()) {\n",
                "      return opt.value();\n",
                "    }\n",
                "    return ExprMutator::VisitExpr_(op);\n",
                "}\n",
                "```\n",
                "\n",
                "The only complex case is handling calls to TIR functions: If there is a call to a TIR function and all the arguments are constants, then the TIR call itself can simply be evaluated at compile time.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "967791fb",
            "metadata": {},
            "source": [
                "## Pass Granularity\n",
                "\n",
                "One of the design points when writing analysis or transformations is to the choose the granularity at which your pass would operate. Relax pass infrastructure enables developers to write passes at either Module, Function, DataflowBlock or TIR PrimFunc level. Choosing the right granularity can make it significantly easier to write your analyses and transformations. \n",
                "\n",
                "#### Module Pass\n",
                "Module-level passes are designed to implement global analysis/optimizations, i.e. interprocedural optimizations (IPO), etc. Passes at this level have the full control of a given Relax IRModule including addition and deletion of functions. For example, [FuseOps pass](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fuse_ops.cc#L776) which together with FuseTIR pass performs fusion in Relax is a module pass as it adds new functions to the IRModule.\n",
                "\n",
                "#### Function Pass:\n",
                "Function level passes are used to implement various intra-function optimizations for a given Relax IRModule. It fetches one function at a time from the function list in the IRModule for optimization. The scope of passes at this level is a Relax function. Therefore, we cannot add or delete a global function through these passes as they are not aware of the global information. For example, [FoldConstant pass](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fold_constant.cc#L206) is a function pass as it performs constant folding within function scope.\n",
                "\n",
                "#### TIR PrimFunc Pass\n",
                "TIR PrimFunc level pass that applies analysis/transformations to all the TIR PrimFunc(s) within the IRModule. It fetches one TIR function at a time from the function list in the IRModule for optimization. The scope of passes at this level is a TIR PrimFunc. Therefore, we cannot add or delete a global function through these passes as they are not aware of the global information. [AnnotateTIROpPattern pass](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/annotate_tir_op_pattern.cc#L43) which annotates op pattern for TIR functions is an example of PrimFunc pass.\n",
                "\n",
                "#### DataflowBlock Pass\n",
                "Dataflow Block level passes are used to implement various dataflow block optimizations for a given Relax IRModule. It fetches one dataflow block at a time from the functions in an IRModule, and returns a rewritten DataflowBlock. The scope of passes at this level is a Relax Dataflow block. Therefore, we cannot modify the global scope Vars and symbolic shape Vars defined inside the dataflow block. All the operations under the dataflow block are side-effect-free and do not contain advanced control flows(such as if-then-else) or nested scopes. We expect most of the optimizations will happen at the dataflow block level and if their changes are local to the dataflow block then the pass developer can choose dataflow block granularity. [FMARewrite pass](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fma_rewrite.cc#L150) is a toy dataflow block pass to perform fused multiply-add rewriting.\n",
                "\n",
                "The following helper functions are provided to create each type of these aforementioned passes in C++. These helpers are also exposed to the Python frontend for users to favorably use Python APIs to create a specific pass object.\n",
                "\n",
                "```c++\n",
                "Pass CreateModulePass(const runtime::TypedPackedFunc<IRModule(IRModule, PassContext)>& pass_func,\n",
                "    int opt_level,\n",
                "    String name,\n",
                "    tvm::Array<String> required, \n",
                "    bool traceable);\n",
                "\n",
                "Pass CreateFunctionPass(\n",
                "    const runtime::TypedPackedFunc<Function(Function, IRModule, PassContext)>& pass_func,\n",
                "    int opt_level, \n",
                "    String name, \n",
                "    tvm::Array<String> \n",
                "    required, \n",
                "    bool traceable);\n",
                "\n",
                "Pass CreatePrimFuncPass(\n",
                "    const runtime::TypedPackedFunc<PrimFunc(PrimFunc, IRModule, PassContext)>& pass_func,\n",
                "    int opt_level, \n",
                "    String name, \n",
                "    tvm::Array<String> required, \n",
                "    bool traceable);\n",
                "\n",
                "Pass CreateDataflowBlockPass(\n",
                "    const runtime::TypedPackedFunc<DataflowBlock(DataflowBlock, IRModule, PassContext)>& pass_func,\n",
                "    int opt_level, \n",
                "    String name, \n",
                "    tvm::Array<String> required, \n",
                "    bool traceable);\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24f4b7ca",
            "metadata": {},
            "source": [
                "## Example: Conv-Conv Fusion\n",
                "\n",
                "In order to understand the process of writing a Relax pass, we will look at a `Conv -> Conv` fusion pass as a guide. It is a *toy* pass that highlights how Relax can be used to perform analysis of high-level graph, fuse the underlying TIR functions, and play around with the schedule.\n",
                "\n",
                "We begin by initializing the TVM environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "e7af6b36",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tvm\n",
                "from tvm import tir, relax, topi\n",
                "from tvm.ir.module import IRModule\n",
                "from tvm.ir.transform import module_pass\n",
                "from tvm.relax import PyExprMutator\n",
                "from tvm.relax.analysis import remove_all_unused\n",
                "from typing import Union\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "067ca544",
            "metadata": {},
            "source": [
                "### Build IRModule with Unfused Convolutions\n",
                "\n",
                "The code below constructs our input IRModule. The IRModule has two convolution operations represented by two successive `R.call_tir(conv2d, ...)` calls in the `main` function. The `conv2d` TIR function performs the convolution operation.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "6c20f945",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We will use the BlockBuilder API to create the input IRModule. We could also\n",
                "# potentially use TVMScript to generate it.\n",
                "bb = relax.BlockBuilder()\n",
                "\n",
                "def build_model(x, w1, w2) -> IRModule:\n",
                "    \n",
                "    # Main function\n",
                "    with bb.function(\"main\", [x, w1, w2]):\n",
                "        with bb.dataflow():\n",
                "            lv1 = bb.emit_te(topi.nn.conv2d, x, w1, strides=1, padding=1, dilation=1)\n",
                "            gv = bb.emit_output(bb.emit_te(topi.nn.conv2d, lv1, w2, strides=1, padding=1, dilation=1))\n",
                "        bb.emit_func_output(gv)\n",
                "    \n",
                "    # Return the IRModule being built\n",
                "    return bb.get()\n",
                "\n",
                "# Input Relax variables.\n",
                "tensor_type = relax.DynTensorType(4, \"float32\")\n",
                "x = relax.Var(\"x\", (1, 16, 64, 64), tensor_type)\n",
                "w1 = relax.Var(\"w1\", (16, 16, 3, 3), tensor_type)\n",
                "w2 = relax.Var(\"w2\", (16, 16, 3, 3), tensor_type)\n",
                "\n",
                "# Get the input IRModule\n",
                "unfused_mod = build_model(x, w1, w2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7fe8164e",
            "metadata": {},
            "source": [
                "Next, we mark the TIR convolution function with the attribute `my_op_kind` = `convolution`. This would help us to identify the convolution operation in our pass later.\n",
                "\n",
                "Note that this is a workaround until Relax has a full operator set. Currently, Relax has a sophisticated [pattern matching infrastructure](https://github.com/tlc-pack/relax/issues/160) to do pattern matching over Relax operators, types, and attributes. However, due to lack of a full operator set, we have to mark TIR functions with attributes in order to identify the underlying operation. For now, Relax pass developers can add their own markers similar to `my_op_kind` to get around this issue. Pattern matching would become much simpler with the addition of full Relax operator set in the future."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "84de1410",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">conv2d</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;conv2d&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;my_op_kind&quot;</span>: <span style=\"color: #BA2121\">&quot;convolution&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], rxplaceholder_1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (x, w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (lv, w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "unfused_mod[\"conv2d\"] = unfused_mod[\"conv2d\"].with_attr(\"my_op_kind\", \"convolution\")\n",
                "unfused_mod.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38ecb8f1",
            "metadata": {},
            "source": [
                "### Fusing Convolutions\n",
                "\n",
                "The goal is to fuse the two successive convolution operations i.e., replace the following in the input IRModule\n",
                "\n",
                "```python\n",
                "lv = R.call_tir(conv2d, (x, w1), (1, 16, 64, 64), dtype=\"float32\")\n",
                "lv1 = R.call_tir(conv2d, (lv, w2), (1, 16, 64, 64), dtype=\"float32\")\n",
                "```\n",
                "\n",
                "with\n",
                "```python\n",
                "lv1 = R.call_tir(grouped_conv_conv, (x, w1, w2), (1, 16, 64, 64), dtype=\"float32\")\n",
                "```\n",
                "where `grouped_conv_conv` is a TIR function that performs the two convolutions.\n",
                "\n",
                "This can be achieved in two steps:\n",
                "\n",
                "__Step 1:__ Group the two convolutions into a *primitive* Relax function. A *primitive* Relax function is one which has a single dataflow block with only `call_tir` bindings inside it. For easier identification, these functions are marked with `Primitive: 1` attribute.\n",
                "\n",
                "__Step 2:__ Use the Relax [FuseTIR pass](https://github.com/tlc-pack/relax/blob/relax/src/relax/transform/fuse_tir.cc) to fuse the called TIR functions in the *primitive* Relax function `grouped_conv_conv` into a single TIR function and further replace all calls to the Relax function `grouped_conv_conv` with a `call_tir` to the fused TIR function.\n",
                "\n",
                "The output of step 1 and step 2 are shown below:\n",
                "\n",
                "__IRModule after Step 1__\n",
                "\n",
                "```python\n",
                "@R.function\n",
                "def main(x: Tensor((1, 16, 64, 64), \"float32\"), w1: Tensor((16, 16, 3, 3), \"float32\"), w2: Tensor((16, 16, 3, 3), \"float32\")) -> Tensor(None, \"float32\", ndim = 4):\n",
                "    # block 0\n",
                "    with R.dataflow():\n",
                "        lv1: Tensor((1, 16, 64, 64), \"float32\") = fused_conv_conv(x, w1, w2)\n",
                "        gv: Tensor((1, 16, 64, 64), \"float32\") = lv1\n",
                "        R.output(gv)\n",
                "    return gv\n",
                "\n",
                "# primitive Relax function    \n",
                "@R.function\n",
                "def grouped_conv_conv(x1: Tensor((1, 16, 64, 64), \"float32\"), w11: Tensor((16, 16, 3, 3), \"float32\"), w21: Tensor((16, 16, 3, 3), \"float32\")) -> Tensor(None, \"float32\", ndim = 4):\n",
                "    # block 0\n",
                "    with R.dataflow():\n",
                "        lv0 = R.call_tir(conv2d, (x1, w11), (1, 16, 64, 64), dtype=\"float32\")\n",
                "        conv_2 = R.call_tir(conv2d, (lv0, w21), (1, 16, 64, 64), dtype=\"float32\")\n",
                "        R.output(conv_2)\n",
                "    return conv_2\n",
                "```\n",
                "\n",
                "__IRModule after Step 2__\n",
                "\n",
                "```python\n",
                "@R.function\n",
                "def main(x: Tensor((1, 16, 64, 64), \"float32\"), w1: Tensor((16, 16, 3, 3), \"float32\"), w2: Tensor((16, 16, 3, 3), \"float32\")) -> Tensor(None, \"float32\", ndim = 4):\n",
                "    # block 0\n",
                "    with R.dataflow():\n",
                "        lv1 = R.call_tir(fused_conv_conv, (x, w1, w2), (1, 16, 64, 64), dtype=\"float32\")\n",
                "        gv: Tensor((1, 16, 64, 64), \"float32\") = lv1\n",
                "        R.output(gv)\n",
                "    return gv\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9a2bb60",
            "metadata": {},
            "source": [
                "\n",
                "### Toy Pass to Group Convolutions (Step 1)\n",
                "\n",
                "Next we write the toy pass to group two successive convolution operations into a *primitive* Relax function. There are a few things to consider here:\n",
                "\n",
                "* __Pass Granularity__: Since our pass would add a new Relax function to the module i.e., the changes are not limited to dataflow block or function scope, it cannot be a function or dataflow block pass. It must be a module pass. *Potentially we could have chosen this to be a dataflow block pass and added a function local to dataflow block scope which could be lifted to module scope later by lambda lifting pass.*\n",
                "* __ExprVisitor or ExprMutator or ExprMutatorBase__: Since it is a transformation pass, we must use either `ExprMutator` or `ExprMutatorBase`. For simplicity, we would like to work with IR in ANF form and not handle all the complexities of non-ANF form. Hence we use `ExprMutator`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "08c3769e",
            "metadata": {},
            "outputs": [],
            "source": [
                "@relax.expr_functor.mutator\n",
                "class GroupTwoConvsMutator(PyExprMutator):\n",
                "\n",
                "    def __init__(self, mod: IRModule) -> None:\n",
                "        # ExprMutator has an internal BlockBuilder `builder_` which keeps an IRModule `context_mod_`\n",
                "        # being built. We can optionally initialize this IRModule with the current module\n",
                "        # with copy-on-write semantics which we can update/add functions to.\n",
                "        super().__init__(mod)\n",
                "        self.mod_ = mod\n",
                "    \n",
                "    # Matches the call_node against the pattern Conv -> Conv pattern. If pattern matches,\n",
                "    # returns the two convolution call nodes, otherwise returns None.\n",
                "    def pattern_match(self, call_node) -> Union[None, relax.Tuple]:\n",
                "        # Helper function to check if the call node op is a `call_tir` operator and\n",
                "        # the called TIR function is a convolution operation.\n",
                "        def is_convolution(call_node: relax.Call) -> bool:\n",
                "            if not isinstance(call_node, relax.Call):\n",
                "                return False\n",
                "            call_tir_op = tvm.ir.Op.get(\"relax.call_tir\")\n",
                "            if call_node.op != call_tir_op:\n",
                "                return False\n",
                "            global_var = call_node.args[0]\n",
                "            tir_func = self.mod_[global_var]\n",
                "            if tir_func.attrs[\"my_op_kind\"] != \"convolution\":\n",
                "                return False\n",
                "            return True\n",
                "        \n",
                "        # Check if the current call_node is a convolution operation.\n",
                "        if not is_convolution(call_node):\n",
                "            return None\n",
                "\n",
                "        # Check if the input tensor to this call_node is also a convolution operation.\n",
                "        operands = call_node.args[1]\n",
                "        input_tensor = operands[0]\n",
                "        value = self.lookup_binding(input_tensor)\n",
                "        if not is_convolution(value):\n",
                "            return None\n",
                "        return [value, call_node]\n",
                "\n",
                "    def transform(self) -> IRModule:\n",
                "        # Iterate over all the functions in the IRModule\n",
                "        for global_var, func in self.mod_.functions.items():\n",
                "            # Skip non-relax functions\n",
                "            if not isinstance(func, relax.Function):\n",
                "                continue\n",
                "            # Skip primitive functions\n",
                "            if \"Primitive\" in func.attrs.keys() and func.attrs[\"Primitive\"] != 0:\n",
                "                continue\n",
                "            # Update the non-primitive Relax function\n",
                "            updated_func = self.visit_expr(func)\n",
                "            # Remove any dead code in the updated function\n",
                "            updated_func = remove_all_unused(updated_func)\n",
                "            self.builder_.update_func(global_var, updated_func)\n",
                "        \n",
                "        # At the end of the transformation we return the updated IRModule from the BlockBuilder.\n",
                "        return self.builder_.get()\n",
                "    \n",
                "    # We only need to override Call node mutator. If this call_node matches \n",
                "    # the Conv->Conv pattern, we can group these calls in a new primitive \n",
                "    # Relax function and replace the current call with call to the primitve \n",
                "    # function.\n",
                "    def visit_call_(self, call_node: relax.Call) -> relax.Call:\n",
                "        # Check if the call node matches our expected pattern\n",
                "        conv_calls = self.pattern_match(call_node)\n",
                "        if not conv_calls:\n",
                "            return call_node\n",
                "        \n",
                "        # Get current vars from convolution calls\n",
                "        x, w1, w2 = conv_calls[0].args[1][0], conv_calls[0].args[1][1], conv_calls[1].args[1][1]\n",
                "\n",
                "        # Construct the parameters of the new function\n",
                "        param_x = relax.Var(\"param_x\", x.shape_, x._checked_type_)\n",
                "        param_w1 = relax.Var(\"param_w1\", w1.shape_, w1._checked_type_)\n",
                "        param_w2 = relax.Var(\"param_w2\", w2.shape_, w2._checked_type_)\n",
                "\n",
                "        # Get the TIR convolution functions\n",
                "        tir_convolution_0 = conv_calls[0].args[0]\n",
                "        tir_convolution_1 = conv_calls[1].args[0]\n",
                "\n",
                "        # Next we construct the primitive function with grouped convolutions\n",
                "        \n",
                "        # First convolution binding\n",
                "        lv0 = relax.DataflowVar(\"lv0\", conv_calls[0].shape_, conv_calls[0]._checked_type_)\n",
                "        conv_1 = relax.call_tir(tir_convolution_0, [param_x, param_w1], conv_calls[0].shape_, dtype=\"float32\")\n",
                "        bindings = [relax.VarBinding(lv0, conv_1)]\n",
                "\n",
                "        # Second convolution binding\n",
                "        gv = relax.Var(\"gv\", conv_calls[1].shape_, conv_calls[1]._checked_type_)\n",
                "        conv_2 = relax.call_tir(tir_convolution_1, [lv0, param_w2], conv_calls[1].shape_, dtype=\"float32\")\n",
                "        bindings.append(relax.VarBinding(gv, conv_2))\n",
                "\n",
                "        block = relax.DataflowBlock(bindings)\n",
                "        seq_expr = relax.SeqExpr([block], gv)\n",
                "        ret_type = conv_calls[1]._checked_type_\n",
                "        func_name = \"grouped_conv_conv\"\n",
                "        grouped_conv_conv = relax.Function([param_x, param_w1, param_w2], seq_expr, ret_type)\n",
                "\n",
                "        # Add global_symbol and Primitive attribute. Later FuseTIR pass would use\n",
                "        # the Primitive attribute to fuse the called TIR functions.\n",
                "        grouped_conv_conv = grouped_conv_conv.with_attr(\"global_symbol\", func_name).with_attr(\"Primitive\", 1)\n",
                "\n",
                "        # Normalize the newly created function and add it to the module\n",
                "        normalized = self.builder_.normalize(grouped_conv_conv)\n",
                "        global_var = self.builder_.add_func(normalized, func_name)\n",
                "\n",
                "        # Construct a call to the primitive function\n",
                "        return relax.Call(global_var, [x, w1, w2], None, None)\n",
                "        \n",
                "\n",
                "@module_pass(opt_level=2, name=\"group_two_conv\")\n",
                "class GroupTwoConvsPass:\n",
                "    \"\"\"The wrapper for the GroupTwoConv pass.\"\"\"\n",
                "\n",
                "    def transform_module(self, mod, ctx):\n",
                "        return GroupTwoConvsMutator(mod).transform()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "0d3b5f0f",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">conv2d</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;conv2d&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;my_op_kind&quot;</span>: <span style=\"color: #BA2121\">&quot;convolution&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, rxplaceholder[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], rxplaceholder_1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> grouped_conv_conv(x, w1, w2)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">grouped_conv_conv</span>(param_x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), param_w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), param_w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (param_x, param_w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(conv2d, (lv0, param_w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv1)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv1\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "grouped_mod = GroupTwoConvsPass()(unfused_mod)\n",
                "if not relax.analysis.well_formed(grouped_mod):\n",
                "    print(\"IRModule is not well-formed\")\n",
                "grouped_mod.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d418bcf",
            "metadata": {},
            "source": [
                "### Fuse Underlying TIR Convolution Functions (Step 2)\n",
                "\n",
                "Now we can simply use the FuseTIR pass to fuse the TIR Convolution functions. *Note that this separation between Relax FuseOps and FuseTIR passes allows us to implement custom fusion strategies (for example conv->conv fusion in this example).* "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "adfda8da",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">grouped_conv_conv</span>(param_x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], param_w1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], param_w2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;grouped_conv_conv&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        conv2d_nchw_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        pad_temp_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(param_x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, param_x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], param_w1[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw_1[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> param_w1[ff, rc, ry, rx]\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp_1&quot;</span>):\n",
                            "                i0_2, i1_2, i2_2, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp_1[i0_2, i1_2, i2_2, i3_2])\n",
                            "                pad_temp_1[i0_2, i1_2, i2_2, i3_2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_2 <span style=\"color: #008000; font-weight: bold\">and</span> i2_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_2 <span style=\"color: #008000; font-weight: bold\">and</span> i3_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0_3, i1_3, i2_3, i3_3, i4, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw_1&quot;</span>):\n",
                            "                nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0_3, i1_3, i2_3, i3_3, i4, i5, i6])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], param_w2[ff, rc, ry, rx])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> param_w2[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(grouped_conv_conv, (x, w1, w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "fused_mod = relax.transform.FuseTIR()(grouped_mod)\n",
                "if not relax.analysis.well_formed(fused_mod):\n",
                "    print(\"IRModule is not well-formed\")\n",
                "fused_mod.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3a022397",
            "metadata": {},
            "source": [
                "## Play with TIR Schedule\n",
                "\n",
                "Next we can play around with schedule of the fused Conv->Conv TIR function. We can use the TIR Schedule API to play around with the schedule. The following code is for demo purpose only and does not necessarily represent an optimized schedule. \n",
                "\n",
                "This gives us the flexibility to change the TIR schedule manually or through a transformation pass and then use the Relax VM build API to compile and run on a target. This enables developers to quickly test out their ideas and check their impact in the end-to-end compilation of the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "796722ca",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
                            "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
                            "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">grouped_conv_conv</span>(param_x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], param_w1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], param_w2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], conv2d_nchw: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
                            "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
                            "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;grouped_conv_conv&quot;</span>})\n",
                            "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
                            "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
                            "        pad_temp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        conv2d_nchw_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        pad_temp_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">66</span>, <span style=\"color: #008000\">66</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
                            "                i0_1, i1_1, i2_1, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(param_x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
                            "                pad_temp[i0_1, i1_1, i2_1, i3_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_1 <span style=\"color: #008000; font-weight: bold\">and</span> i2_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_1 <span style=\"color: #008000; font-weight: bold\">and</span> i3_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, param_x[i0_1, i1_1, i2_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_1 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i2, i3, i5, i6 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
                            "                    nn <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>)\n",
                            "                    ff <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, ax0)\n",
                            "                    yy <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">64</span>, i2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i5 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>)\n",
                            "                    xx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">64</span>, i3 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i6 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>)\n",
                            "                    rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;RRR&quot;</span>, [ax1, ax2, ax3])\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i5 <span style=\"color: #008000; font-weight: bold\">and</span> i2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i5 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i6 <span style=\"color: #008000; font-weight: bold\">and</span> i3 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i6 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>)\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], param_w1[ff, rc, ry, rx])\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw_1[nn, ff, yy, xx])\n",
                            "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                        conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                    conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw_1[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> param_w1[ff, rc, ry, rx]\n",
                            "            <span style=\"color: #008000; font-weight: bold\">for</span> i1, i4 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp_1&quot;</span>):\n",
                            "                    i0_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>)\n",
                            "                    i1_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, i4)\n",
                            "                    i2_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">66</span>, i2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i5)\n",
                            "                    i3_2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">66</span>, i3 <span style=\"color: #AA22FF; font-weight: bold\">+</span> i6)\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_temp_1[i0_2, i1_2, i2_2, i3_2])\n",
                            "                    pad_temp_1[i0_2, i1_2, i2_2, i3_2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2_2 <span style=\"color: #008000; font-weight: bold\">and</span> i2_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i3_2 <span style=\"color: #008000; font-weight: bold\">and</span> i3_2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">65</span>, conv2d_nchw_1[i0_2, i1_2, i2_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, i3_2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw_1&quot;</span>):\n",
                            "                    nn, ff, yy, xx, rc, ry, rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [i0, i1, i2, i3, i4, i5, i6])\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx], param_w2[ff, rc, ry, rx])\n",
                            "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(conv2d_nchw[nn, ff, yy, xx])\n",
                            "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
                            "                        conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
                            "                    conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">=</span> conv2d_nchw[nn, ff, yy, xx] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_temp_1[nn, rc, yy <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry, xx <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> param_w2[ff, rc, ry, rx]\n",
                            "    \n",
                            "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
                            "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w2: Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">4</span>):\n",
                            "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
                            "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
                            "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(grouped_conv_conv, (x, w1, w2), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
                            "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
                            "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
                            "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
                            "    \n",
                            "</pre></div>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "sch = tvm.tir.Schedule(fused_mod)\n",
                "sch.work_on(\"grouped_conv_conv\")\n",
                "\n",
                "# Play around with loop reordering. This is for demo purpose only and\n",
                "# does not represent a more optimized loop order.\n",
                "conv1_block = sch.get_block(\"conv2d_nchw\")\n",
                "conv2_block = sch.get_block(\"conv2d_nchw_1\")\n",
                "c1_n, c1_co, c1_y, c1_x, c1_ci, c1_ky, c1_kx = sch.get_loops(conv1_block)\n",
                "c2_n, c2_co, c2_y, c2_x, c2_ci, c2_ky, c2_kx = sch.get_loops(conv2_block)\n",
                "sch.reorder(c1_n, c1_y, c1_x, c1_ky, c1_kx, c1_co, c1_ci)\n",
                "sch.reorder(c2_n, c2_y, c2_x, c2_ky, c2_kx, c2_co, c2_ci)\n",
                "pad_block_1 = sch.get_block(\"pad_temp\")\n",
                "pad_block_2 = sch.get_block(\"pad_temp_1\")\n",
                "sch.compute_at(pad_block_2, c2_ci)\n",
                "sch.compute_at(conv1_block, c2_kx)\n",
                "\n",
                "# Let's see what the modified module looks like\n",
                "sch.mod.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "892ff5a1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile and run\n",
                "x = tvm.nd.array(np.random.normal(size=[1, 16, 64, 64]).astype(\"float32\"))\n",
                "w1 = tvm.nd.array(np.random.normal(size=[16, 16, 3, 3]).astype(\"float32\"))\n",
                "w2 = tvm.nd.array(np.random.normal(size=[16, 16, 3, 3]).astype(\"float32\"))\n",
                "\n",
                "# build the IRModule and create relax vm\n",
                "target = tvm.target.Target(\"llvm\")\n",
                "ex = relax.vm.build(sch.mod, target)\n",
                "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
                "\n",
                "res = vm[\"main\"](x, w1, w2)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "fc43b24afbafa24450c433caa80e216064e86ab300af4b30503108fd4e5dd3a0"
        },
        "kernelspec": {
            "display_name": "Python 3.7.11 ('py37')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
