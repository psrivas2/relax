{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # must import to defer parsing of annotations\n",
    "import os\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.relay import Call\n",
    "from tvm import relax, tir, topi\n",
    "from tvm.runtime import container\n",
    "from tvm.relax.testing import nn\n",
    "\n",
    "import tvm.script\n",
    "from tvm.script import tir as T, relax as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = relax.BlockBuilder()\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 32]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with builder.function(name=\"main\"):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size),\n",
    "            nn.LogSoftmax(),\n",
    "        )\n",
    "        # n is a symbolic variable to represent a dynamic batch size\n",
    "        n = tir.Var(\"n\", \"int64\")\n",
    "        data = nn.Placeholder((n, input_size), name=\"data\")\n",
    "        output = model(data)\n",
    "        params = [data] + model.parameters()\n",
    "        builder.emit_func_output(output, params=params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = builder.get()\n",
    "print(R.parser.astext(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the input data: :math:`y = xA + b`.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter((in_features, out_features), name=\"linear_weight\")\n",
    "        if bias:\n",
    "            self.bias = Parameter((out_features,), name=\"linear_bias\")\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input: relax.Expr) -> relax.Var:\n",
    "        y = emit_te(topi.matmul, input, self.weight)\n",
    "        if self.bias is not None:\n",
    "            y = emit_te(topi.add, y, self.bias)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(data, weight):\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    with bb.function(\"mlp\", [data, weight]):\n",
    "        gv0 = bb.emit_te(tvm.contrib.cblas.matmul, data, weight, transa=False, transb=False)\n",
    "        gv1 = bb.emit_te(topi.nn.relu, gv0)\n",
    "        bb.emit_func_output(gv1)\n",
    "\n",
    "    mod = bb.get()\n",
    "    return mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbolic dimensions\n",
    "n, m = tir.Var(\"n\", \"int64\"), tir.Var(\"m\", \"int64\")\n",
    "\n",
    "# create data and weight variables\n",
    "data = relax.Var(\"data\", [n, m], relax.DynTensorType(2, \"float32\"))\n",
    "weight = relax.Var(\"weight\", [m, n], relax.DynTensorType(2, \"float32\"))\n",
    "\n",
    "# construct a mlp model\n",
    "mod = build_mlp(data, weight)\n",
    "print(R.parser.astext(mod))\n",
    "\n",
    "# build and create vm executor\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "ex = relax.vm.build(mod, target)\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the mlp model on relax vm\n",
    "data = tvm.nd.array(np.random.rand(16, 32).astype(np.float32))\n",
    "weight = tvm.nd.array(np.random.rand(32, 16).astype(np.float32))\n",
    "res = vm[\"mlp\"](data, weight)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.register_func(\"test.vm.tile\")\n",
    "def tile_packed(a, b):\n",
    "    b[:] = tvm.nd.array(np.tile(a.asnumpy(), (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"\"\"@tvm.script.ir_module\n",
    "class InputModule:\n",
    "    @R.function\n",
    "    def foo(x: Tensor[(n, m), \"float32\"]) -> Tensor:\n",
    "        with relax.dataflow():\n",
    "            y = R.call_tir(\"test.vm.tile\", (x), (n, m * 2), dtype=\"float32\")\n",
    "            relax.output(y)\n",
    "        return y\n",
    "\"\"\"\n",
    "\n",
    "# Original Relax Program\n",
    "print(\"======================\")\n",
    "print(\"Original Relax Program\\n\")\n",
    "mod = R.parser.from_source(src)\n",
    "code = R.parser.astext(mod)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToNonDataflow Pass\n",
    "print(\"======================\")\n",
    "print(\"PASS0: To Non Dataflow\\n\")\n",
    "mod = relax.transform.ToNonDataflow()(mod)\n",
    "print(R.parser.astext(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CallDPS Rewrite\n",
    "print(\"======================\")\n",
    "print(\"PASS1: CallDPS Rewrite\\n\")\n",
    "mod = relax.transform.CallTIRRewrite()(mod)\n",
    "print(R.parser.astext(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Lower\n",
    "print(\"======================\")\n",
    "print(\"PASS2: Memory Lower\\n\")\n",
    "mod = relax.transform.VMMemoryLower()(mod)\n",
    "print(R.parser.astext(mod))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Lower\n",
    "print(\"======================\")\n",
    "print(\"PASS3: Shape Lower\\n\")\n",
    "mod = relax.transform.VMShapeLower()(mod)\n",
    "print(R.parser.astext(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & Execute\n",
    "print(\"======================\")\n",
    "print(\"Build & Execute\")\n",
    "\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "ex = relax.vm.build(mod, target)\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "shape = (3, 4)\n",
    "inp = tvm.nd.array(np.random.rand(*shape).astype(np.float32))\n",
    "out = vm[\"foo\"](inp)\n",
    "print(\"input: \", inp)\n",
    "print(\"output: \", out)\n",
    "np.testing.assert_allclose(np.tile(inp.asnumpy(), (1, 2)), out.asnumpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc43b24afbafa24450c433caa80e216064e86ab300af4b30503108fd4e5dd3a0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
